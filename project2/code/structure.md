# Structure of Neural Network

### Classes (tentative plan):

`layer.py`

* Subclasses

`network.py`

`activation_function.py`

`cost_functions`

`GradientDescent`

Can we use taylor expansion and differentiate with auto grad for our output activation function? No i dont think so.

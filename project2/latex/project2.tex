\input{preamble.tex}

\begin{document}



\title{Classification and Regression: \\
From Linear and Logistic Regression to Neural Network} 

\author{Johan Mylius Kroken
\inst{1,2},
\and
Nanna Bryne
\inst{1,2}
}
\institute{Institute of Theoretical Astrophysics (ITA), University of Oslo, Norway
\and
Center for Computing in Science Education (CCSE), University of Oslo, Norway}
%\email{nanna.bryne@fys.uio.no}}
\titlerunning{Cool title}
\authorrunning{Kroken\and Bryne} 
\date{\today    \quad GitHub repo link: \url{https://github.com/Johanmkr/FYS-STK4155colab/tree/main/project2}}  
\abstract{
ABSTRACT
}

\maketitle


\bibliographystyle{../../aa}

\par $\quad$ 
\par \noindent ************************************************

\feltcute{for ideas (\textbackslash feltcute)}

\rephrase{rephrase this (\textbackslash rephrase\{...\})}

\checkthis{check if this is correct (\textbackslash checkthis\{...\})}

\comment{comment (\textbackslash comment\{...\})}

\fillertext[(\textbackslash fillertext)]

\wtf[for when you are lost (\textbackslash wtf)]

\par \noindent ************************************************
\par $\quad$ 
\par $\quad$ 


\section*{Nomenclature}
\noindent\subsection{Datasets ??}
\begin{itemize}
    \item[$\svec{\theta}$] Parameter vector
    \item[$\mathcal{L}$] \rephrase{Total loss/cost function $\mathcal{L}(\svec{\theta}; f, \mathcal{D})$ of $\svec{\theta}$ parametrised by the coordinates in the data set $\mathcal{D}$ and the function $f$ (often written as $\mathcal{L}(\svec{\theta})$ for ease of notation)}
    \item[$\mathcal{A}$] Magnitude and direction of steepest ascent in parameter space
    \item[$X$] Feature matrix of $n$ row vectors $\vec{x}^{(i)} \in \RR[p]$, where $p$ denotes the number of features we are considering
    \item[$\vec{y}$] Vector of $n$ input targets $y^{(i)} \in \RR$ associated with $\vec{x}^{(i)}$
    \item[$\mathcal{D}$] Dataset $\big\{ X, \vec{y} \big\}$ of length $n\in \mathbb{N}$ on the form $\big\{(\vec{x}^{(1)}, y^{(1)}),\,(\vec{x}^{(2)}, y^{(2)}),\,\dots, \, (\vec{x}^{(n)}, y^{(n)}) \big\} $
    \item[$n$] Number of samples in a datasets  
\end{itemize}
\subsection{Network components}
\begin{itemize}
    \item[$\vec{h}$] Hidden layer of a neural network
    \item[$g$] Activation function associated with a layer in a neural network, affine transformation \checkthis{$\RR[xxx]\to\RR[yyy]$}
    \item[$W$] Matrix of weights describing the mapping from a layer to the next
    \item[$\vec{b}$] Bias \comment{More!}
\end{itemize}

\subsection{Hyperparameter syntax}
\begin{itemize}
    \item[$\eta$] Learning rate
    \item[$\gamma$] Momentum factor
    \item[$\vec{v}$] Momentum in parameter space  
    \item[$L$] Number of layers, not counting the input 
    \item[$\lambda$] Penalty parameter in Ridge regression 
\end{itemize}


\subsection{Indexing and iteration variables}
\begin{itemize}
    \item[$k$] Iteration variable when optimising as \textbf{subscript}
    \item[$(i)$] The $i^\mathrm{th}$ example of a sample as \textbf{superscript}
\end{itemize}


\subsection{Miscellaneous}

\begin{enumerate}[leftmargin=2.1em]
    \item[$\norm{\vec{u}}_q$] \lnorm[q]\, of $\vec{u}$
    \item[$\nabla_{\!\xi} \rho$] gradient of $\rho$ with respect to $\xi$
\end{enumerate}

\subsection{Forkortelser p√• engelsk}
\begin{enumerate}[leftmargin=2.6em]
    \item[DAG] Directed acyclic graph
    \item[FFNN] Feedforward neural network
    \item[GD] Gradient descent
    \item[MSE] Mean squared error 
    \item[NAG] Nesterov accelerated gradient
    \item[NN] Neural network 
    \item[OLS] Ordinary least squares 
    \item[ReLU] Rectified linear unit
    \item[SGD] Stochastic gradient descent 
\end{enumerate}


\comment{Should order alphabetically or logically.}

%\tableofcontents
\input{introduction}
\input{theory}
\input{analysis}
\input{conclusion}

\section*{Code availability}
The code is available on GitHub at \url{https://github.com/Johanmkr/FYS-STK4155colab/tree/main/project2}.

%\newpage
%\listoffigures

\bibliography{ref}

\end{document}

\input{preamble.tex}

\begin{document}



\title{Classification and regression: \\
From linear and logistic regression to neural network} 

\author{Johan Mylius Kroken
\inst{1,2}
\and
Nanna Bryne
\inst{1,2}
}
\institute{Institute of Theoretical Astrophysics (ITA), University of Oslo, Norway
\and
Center for Computing in Science Education (CCSE), University of Oslo, Norway}
%\email{nanna.bryne@fys.uio.no}}
\titlerunning{Cool title}
\authorrunning{Kroken\and Bryne} 
\date{\today    \quad GitHub repo link: \url{\projectTwolink}}  
\abstract{
We build a versatile neural network code in order to perform linear regression and binary classification tasks. We train the network by minising the loss function by performing plain and stochastic gradient descent (SGD) for a variety of optimisation algorithms. SGD with RMSProp optimiser perform best and is used in training. A network with 1 hidden layer of 30 neurons where $\eta=10^{-1}$ and $\lambda=10^{-4}$ which uses the sigmoid activation function trained for 700 epochs with 2 mini batches yield the best test MSE of 0.052 when trained to fit the noise Franke function, compared to an MSE of 0.15 for OLS. For the binary classification task the data is the Wisconsin Breast Cancer data. A neural network of 2 hidden layers of 10 neurons each where $\eta=10^{-3}$ and $\lambda=10^{-6}$ which uses the ReLU activation function trained for 900 epochs with 5 mini batches yield the best accuracy of 1. Logistic regression with $\eta=10^{-3}$ and $\lambda=10^{-8}$ also yield an accuracy of 1. 
}

\maketitle


\bibliographystyle{../../aa}

% \par $\quad$ 
% \par \noindent ************************************************

% \feltcute{for ideas (\textbackslash feltcute)}

% \rephrase{rephrase this (\textbackslash rephrase\{...\})}

% \checkthis{check if this is correct (\textbackslash checkthis\{...\})}

% \comment{comment (\textbackslash comment\{...\})}

% \fillertext[(\textbackslash fillertext)]

% \wtf[for when you are lost (\textbackslash wtf)]

% \par \noindent ************************************************
% \par $\quad$ 
% \par $\quad$ 


\section*{Notation and nomenclature}

\checkthis{Nanna will fix this towards the end}
\noindent\subsection{Datasets ??}
\begin{itemize}
    \item[$\svec{\theta}$] Parameter vector
    \item[$\mathcal{L}$] \rephrase{Total loss/cost function $\mathcal{L}(\svec{\theta}; f, \mathcal{D})$ of $\svec{\theta}$ parametrised by the coordinates in the data set $\mathcal{D}$ and the function $f$ (often written as $\mathcal{L}(\svec{\theta})$ for ease of notation)}
    \item[$\mathcal{A}$] Magnitude and direction of steepest ascent in parameter space
    \item[$X$] Feature matrix of $n$ row vectors $\vec{x}^{(i)} \in \RR[p]$, where $p$ denotes the number of features we are considering
    \item[$\vec{y}$] Vector of $n$ input targets $y^{(i)} \in \RR$ associated with $\vec{x}^{(i)}$
    \item[$\mathcal{D}$] Dataset $\big\{ X, \vec{y} \big\}$ of length $n\in \mathbb{N}$ on the form $\big\{(\vec{x}^{(1)}, y^{(1)}),\,(\vec{x}^{(2)}, y^{(2)}),\,\dots, \, (\vec{x}^{(n)}, y^{(n)}) \big\} $
    \item[$n$] Number of samples in a datasets  
\end{itemize}
\subsection{Network components}
\begin{itemize}
    \item[$\vec{h}$] Hidden layer of a neural network ($\vec{h}^l \in \RR[N_l]$)
    \item[$g$] Activation function associated with a layer in a neural network, affine transformation ($g_l \,:\,\RR[N_l] \to\RR[N_{l}]$)
    \item[$W$] Matrix of weights describing the mapping from a layer to the next ($W^{l\to l+\! 1} \in \RR[N_l \cross N_{l+\! 1}]$)
    \item[$\vec{b}$] Bias ($\vec{b}^l \in \RR[N_l]$) \comment{More!}
    \item[$\vec{a}$] \checkthis{Activation argument} ($\vec{a}^l \in \RR[N_l]$)
    \item[$N$] Number of neurons in a layer ($N \in \mathbb{N}$)
\end{itemize}

\subsection{Hyperparameter syntax}
\begin{itemize}
    \item[$\eta$] Learning rate
    \item[$\gamma$] Momentum factor
    \item[$\vec{v}$] Momentum in parameter space  
    \item[$L$] Number of layers, not counting the input 
    \item[$\lambda$] Regularisation parameter (penalty parameter in Ridge regression)
    \item[$m$] Number of minibatcher
\end{itemize}


\subsection{Indexing and iteration variables}
\begin{itemize}
    \item[$k$] Iteration variable when optimising as \textbf{subscript}
    \item[$(i)$] The $i^\mathrm{th}$ example of a sample as \textbf{superscript}
\end{itemize}


\subsection{Miscellaneous}

\begin{enumerate}[leftmargin=2.1em]
    \item[$\norm{\vec{u}}_q$] \lnorm[q]\, of $\vec{u}$
    \item[$\nabla_{\!\xi} J$] Gradient of $J$ with respect to $\svec{\xi}$
    \item[$\mathcal{N}(\mu, \sigma)$]  Normal distribution with mean $\mu$ and standard deviation $\sigma$
\end{enumerate}

\subsection{Acronyms}
\begin{enumerate}[leftmargin=2.6em]
    \item[DAG] Directed acyclic graph
    \item[FFNN] Feedforward neural network
    \item[GD] Gradient descent
    \item[MSE] Mean squared error 
    \item[NAG] Nesterov accelerated gradient
    \item[NN] Neural network 
    \item[OLS] Ordinary least squares 
    \item[ReLU] Rectified linear unit
    \item[SGD] Stochastic gradient descent 
\end{enumerate}


\comment{Should order alphabetically or logically.}

%\tableofcontents
\input{introduction}
\input{theory}
\input{analysis}
\input{conclusion}

\section*{Code availability}
The code is available on GitHub at \url{\projectTwolink}.

%\newpage
%\listoffigures

\bibliography{ref}

\input{appendix.tex}

\end{document}

\input{preamble.tex}

\begin{document}



\title{Classification and regression: \\
From linear and logistic regression to neural network} 

\author{Nanna Bryne
\inst{1,2}
\and
Johan Mylius Kroken
\inst{1,2}
}
\institute{Institute of Theoretical Astrophysics (ITA), University of Oslo, Norway
\and
Center for Computing in Science Education (CCSE), University of Oslo, Norway}
%\email{nanna.bryne@fys.uio.no}}
\titlerunning{From linear and logistic regression to neural network}
\authorrunning{Bryne\and Kroken} 
\date{\today    \quad GitHub repo link: \url{\projectTwolink}}  
\abstract{
We build a versatile neural network code in order to perform linear regression and binary classification tasks. We train the network by minising the loss function by performing plain and stochastic gradient descent (SGD) for a variety of optimisation algorithms. SGD with RMSProp optimiser perform best and is used in training. A network with 1 hidden layer of 30 neurons where $\eta=10^{-1}$ and $\lambda=10^{-4}$ which uses the sigmoid activation function trained for 700 epochs with 2 minibatches yield the best test MSE of 0.052 when trained to fit the noise Franke function, compared to an MSE of 0.15 for OLS. For the binary classification task the data is the Wisconsin Breast Cancer data. A neural network of 2 hidden layers of 10 neurons each where $\eta=10^{-3}$ and $\lambda=10^{-6}$ which uses the ReLU activation function trained for 900 epochs with 5 minibatches yield the best accuracy of 1. Logistic regression with $\eta=10^{-3}$ and $\lambda=10^{-8}$ also yield an accuracy of 1. 
}

\maketitle


\bibliographystyle{../../aa}

% \par $\quad$ 
% \par \noindent ************************************************

% \feltcute{for ideas (\textbackslash feltcute)}

% \rephrase{rephrase this (\textbackslash rephrase\{...\})}

% \checkthis{check if this is correct (\textbackslash checkthis\{...\})}

% \comment{comment (\textbackslash comment\{...\})}

% \fillertext[(\textbackslash fillertext)]

% \wtf[for when you are lost (\textbackslash wtf)]

% \par \noindent ************************************************
% \par $\quad$ 
% \par $\quad$ 

\tableofcontents
\section*{Notation and nomenclature}

\subsection*{Datasets and outputs} % working title
\begin{itemize}[leftmargin=2.5em]
    \item[$\mathcal{D}$] Dataset $\big\{ X, \vec{y} \big\}$ of length $n$ on the form $\big\{(\vec{x}^{(1)}, y^{(1)}),\,(\vec{x}^{(2)}, y^{(2)}),\,\dots, \, (\vec{x}^{(n)}, y^{(n)}) \big\} $
    \item[$f$] Function $f(\svec{\theta}; \vec{x})$ that gives the model's output $\hat{y}$ given some input $\vec{x}$
    \item[$\mathcal{L}$] Total loss function $\mathcal{L}(\hat{y}, y)$ where $\hat{y}= f(\svec{\theta}; \vec{x})$ is the output from our model (often written as $\mathcal{L}(\svec{\theta})$ for ease of notation)
    \item[$n$] $\in \mathbb{N}\,;$ Number of samples in a dataset
    \item[$p$] $\in \mathbb{N}\,;$ Number of features of the dependent variables in a dataset
    \item[$\vec{x}^{(i)}$] $\in\RR[p]\, ;$ The $i^\mathrm{th}$ example of the dependent variable $\vec{x}$
    \item[$X$] $\in\RR[n\cross p]\, ;$ Feature matrix of $n$ row vectors $\vec{x}$
    \item[$y^{(i)}$] $\in \RR\,;$ The $i^\mathrm{th}$ example of the independent variable $y$ associated with $\vec{x}^{(i)}$
    \item[$\hat{y}^{(i)}$] $\in \RR\,;$ The output associated with $(\vec{x}^{(i)}, y^{(i)}) \in \mathcal{D}$ 
    \item[$\svec{\theta}$] $\in\RR[p]\, ;$ Parameter vector, or vector of coefficients 
\end{itemize}
\subsection*{Syntax for steepest descent algorithms}
\begin{itemize}
    \item[$\vec{v}$] $\in\RR[p]\, ;$ Momentum in parameter space ($\vec{v}_k$ denotes the last update to $\svec{\theta}_k$)
    \item[$\mathcal{A}$] $=\nabla_{\! \theta} \mathcal{L} \in\RR[p]\, ;$ Magnitude and direction of steepest ascent in parameter space ($\mathcal{A}_k = \nabla_{\! \theta} \mathcal{L}(\svec{\theta}_k)$)
\end{itemize}

\subsection*{Network components}
\begin{itemize}[leftmargin=3.6em]
    \item[$\vec{a}$] $\in\RR[N_l]\, ;$ Activation argument of layer $l$
    \item[$\vec{b}^l$] $\in \RR[N_l]\,;$ Bias term of neurons in layer $l$
    \item[$g_l$] $:\,\RR[N_l] \to\RR[N_{l}]\,;$ Activation function associated with layer $l$, an affine transformation
    \item[$\vec{h}^l$] $\in\RR[N_l]\, ;$ Information associated with layer $l$ 
    \item[$N_l$] $\in \mathbb{N}$ Number of neurons in layer $l$
    \item[$W^{l\to l+\! 1}$] $\in \RR[N_l \cross N_{l+\! 1}]\,;$ Matrix of weights describing the mapping from layer $l$ to layer $l+1$ ($W^{l+\! 1 \leftarrow l} \equiv [W^{l\to l+\! 1}]\TT$)
\end{itemize}

\subsection*{Hyperparameter syntax}
\begin{itemize}[leftmargin=2.8em]
    \item[$L$] Number of layers in an NN, not counting the input layer, or label of output layer 
    \item[$m$] Number of minibatches in SGD
    \item[$\gamma$] Momentum factor (constant term)
    \item[$\eta$] Learning rate (global)
    \item[$\lambda$] Regularisation parameter (penalty parameter in Ridge regression)
    \item[$\rho_1, \rho_2$] Hyperparameters related to RMSProp ($\rho \equiv \rho_2$) and Adam
\end{itemize}

\subsection*{Miscellaneous}
\begin{enumerate}[leftmargin=4.1em]
    \item[$\mathcal{N}(\mu, \sigma)$]  Normal distribution with mean $\mu$ and standard deviation $\sigma$
    \item[VAR] Variance of $A$ \checkthis{FIX!!}
    \item[$\nabla_{\!\xi} J$] Gradient of a function $J$ with respect to $\svec{\xi}$
\end{enumerate}

\subsection*{Acronyms}
\begin{enumerate}[leftmargin=3.3em]
    \item[DAG] Directed acyclic graph
    \item[FFNN] Feedforward neural network
    \item[GD] Gradient descent
    \item[MSE] Mean squared error 
    \item[NN] Neural network 
    \item[OLS] Ordinary least squares 
    \item[ReLU] Rectified linear unit
    \item[SGD] Stochastic gradient descent 
\end{enumerate}



\input{introduction}
\input{theory}
\input{analysis}
\input{conclusion}

\section*{Code availability}
The code is available on GitHub at \url{\projectTwolink}.

%\newpage
%\listoffigures

\bibliography{ref}

\input{appendix.tex}

\end{document}

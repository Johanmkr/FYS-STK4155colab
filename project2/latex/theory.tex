\section{Theory}\label{sec:theory}

In linear regression we have the famous assumption that 

\begin{align}
    y_i = f(\vec{x}_i) + \epsilon_i \simeq \vec{x}_i\TT \svec{\beta} + \epsilon_i,
\end{align}

where $f$ is a continous funtion of $\vec{x}$. \checkthis{If we now were to allow said function to represent discrete outputs, we would benefit from moving on to \textit{logsitic} regression.} \comment{Maybe move to intro?}

\fillertext[smooth transition to steepest descent]

\subsection{Stochastic gradient descent (SGD)}\label{sec:stochastic_gradient_descent}
    Gradient descent \citep{mhjensen} \fillertext[aim is to find beta etc]

    The result is a flexible way to locate the minima of \checkthis{any} cost function $\mathcal{L}(\svec{\theta})$. The ordinary least squares and Ridge schemes of linear regression that we discussed in \href{https://github.com/Johanmkr/FYS-STK4155colab/tree/main/project1}{project 1}, are then implemented by using the cost functions $\mathcal{L}^\mathrm{OLS}(\svec{\theta})$ and $\mathcal{L}^\mathrm{Ridge}(\svec{\theta})$ respectively, given by the following expressions:

    \begin{subequations}\label{eq:linear_regression_cost_functions}
        \begin{align}
            \mathcal{L}^\mathrm{OLS}(\svec{\theta}) &= \norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2 \label{eq:ols_cost_function}\\
            \mathcal{L}^\mathrm{Ridge}(\svec{\theta})&=\norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2  -  \lambda\norm{\svec{\theta}}_2^2 \label{eq:ridge_cost_function}
        \end{align}
    \end{subequations}

    \comment{Explain different parts}

    \subsubsection{Plain gradient descent (GD)}\label{sec:plain_gradient_descent}
        The most basic concept is that of steepest descent. In order to find a minimum of a function $\rho(\svec{\xi})$ (allow multivariability of $\svec{\xi}$), we follow the steepest descent of that function, i.e. the direction of the negative gradient $-\nabla_{\!\xi} \rho(\svec{\xi})$. We thus have an iterative scheme to find minima:
        \begin{align}\label{eq:steepest_descent}
            \svec{\xi}_{k+1} = \svec{\xi}_k - \eta_k\nabla_{\!\xi} \rho(\svec{\xi}_k),
        \end{align}
        where $\eta_k$ may be referred to as the step length or learning rate, which \fillertext

        What we would like to minimise is the cost function $\mathcal{L}(\svec{\theta})$ which is a function of the parameters $\svec{\theta}$ which we are trying to estimate. This means that \Eq{steepest_descent} translates into:
        \begin{align}\label{eq:steepest_descent_cost_function}
            \mathcal{A}_k &\equiv \nabla_{\!\theta} \mathcal{L}(\svec{\theta}_k) \nonumber \\
            \vec{v}_k &= -\eta_k\mathcal{A}_k \nonumber \\
            \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k
        \end{align}
        Where we define $\mathcal{A}_k$ to be the direction and magnitude of the steepest ascent in parameter space. 
        For a sufficiently small $\eta_k$, this method will converge to a minimum of $\svec{\theta}$. (However, since we may not know the nature of $\svec{\theta}$ ,there is a risk that this is just a local and not a global minimum. The steepest descent method in \Eq{steepest_descent_cost_function} is a deterministic method, which means we may get stuck in a local minimum. To avoid this we introduce stochastic gradient descent.) 

    \subsubsection{Momentum}\label{sec:momentum}
        From \Eq{steepest_descent_cost_function} we have that the movement in parameter space is given by $\vec{v}_k$ (the negative of which), which describes the direction and magnitude of the steepest ascent in parameter space. Sometimes we might want to move larger distances in one step. This can be achieved by introduction momentum, where we add an addition term to $\vec{v}_k$:
        \begin{align}
            \vec{v}_k &= \gamma\vec{v}_{k-1} - \eta_k\mathcal{A}_k \nonumber \\
            \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k
        \end{align}
        where $\gamma$ is a momentum parameter and $\eta_k$ is the same learning parameter as before. The basic idea is that this with this method we "overshoot" the descending step length in the direction of the previous step, with a magnitude that is controlled by $\gamma$. By doing this, we may reach the desired minimum with fewer iterations. 

        \comment{ADD NAG}
        \begin{align}\label{eq:nag}
            \mathcal{A}_k \to \mathcal{A}_k(\svec{\theta}_k + \gamma\vec{v}_{k-1}) \implies \mathcal{A}_k= \nabla_{\!\theta}\mathcal{L}(\svec{\theta}_k + \gamma\vec{v}_{k-1})
        \end{align}

    \subsubsection{Stochasticity}\label{sec:stochasticity}
        There are several weaknesses to the plain gradient descent, perhaps the largest is the computational expense of on large datasets and its sensitivity of initial conditions and learning rates. If $\svec{\theta}$ have numerous local minima, we will find one minimum only per set of initial conditions, and we have no good way of saying whether this minimum is global or not. One way of overcoming this is by adding stochasticity to the gradient descent algorithm. 

        The main idea is that we have $n$ data points, that we divide into $M$ minibatches (subsets), meaning that we have $n/M$ data points in each minibatch, denoted $B_j$ for $j\in\{1,2,\dots,n/M\}$. We also recognize that we may write the total cost function as a sum over all data points $\vec{x}^{(i)}$ for $i\in[1,n]$, 
        \begin{equation}
            \mathcal{L}(\svec{\theta}) =\sum_{(\vec{x}, y)\in \mathcal{D}} l(f(\vec{x};\svec{\theta}), y) = \sum_{i=1}^n l_i(\svec{\theta}),
        \end{equation}

        where $l_i = l(f(\vec{x}^{(i)};\svec{\theta}); y^{(i)})$, and thus approximate the gradient of the cost function by only summing over the data points in a minibatch picked at random:
        \begin{align}
            \nabla_{\!\theta} \mathcal{L}(\svec{\theta}) &= \sum_{i=1}^n \nabla_{\!\theta}l_i(\svec{\theta}) \\
            \mathcal{A}^j_k &= \sum_{i\in B_j}\nabla_{\!\theta} l_i(\svec{\theta}_k)
        \end{align}

        
    
    \subsubsection{Tuning}\label{sec:tuning}
        \comment{Something about hyper parameters $\lambda$ and $\eta$}

\subsection{Neural Network (NN)}\label{sec:neural_network}

    \subsubsection{Basics}\label{sec:basics}

    A feedforward NN (FFNN) is typically built by composing together several functions into a chain of function. Associated with this model is a directed acyclic graph (DAG) describing the explicit structure. The depth of the model is determined by the length of the abovementioned chain. Each function represents a layer in the network. The final layer of an FFNN is the output layer, and the layers between the input (prior to the first) and the output layer are called hidden layers. \citep{Goodfellow2016}

    The structure of such a chain-based architecture is described by the $L-1$ hidden layers $\vec{h}^l,\,l=1,2, \dots, L-1$, given by

    \begin{subequations}
        \begin{align} 
            \vec{h}^0 &= \vec{x}\,; \\% dunno
            \vec{h}^1 &= g^1 \big((W^1)\TT \vec{h}^0 + \vec{b}^1\big)\,; \\
            \vec{h}^2 &= g^2 \big( (W^2)\TT \vec{h}^1 + \vec{b}^2\big)\,; \\
            &\vdots \nonumber \\
            \vec{h}^{L} &= g^L \big((W^{L})\TT \vec{h}^{L-1} + \vec{b}^L\big)\,;             
        \end{align}
    \end{subequations}

    where we defined $\vec{h}^0$ and $\vec{h}^L$ to be the input and output layer, respectively. The activation function $g$ \fillertext

    We will build our FFNN (\ref{item:build1}-\ref{item:build3}) and solve a supervised learning problem (\ref{item:solve1}-\ref{item:solve3}) using the steps listed below \citep{mhjensen}.

    \begin{enumerate}[label=(\roman*)]
        \item\label{item:build1} Collect and prepocess data, that is we extract 80\% of the dataset and reserve the rest for validation. \comment{Do we scale???}
        \item\label{item:build2} Define the model and design its architecture. In practice, this means to decide on hyperparameters of the NN such as depth ($L$), 
        \item\label{item:build3} Choose loss function and optimiser \wtf[please send help]
        \item\label{item:solve1} Train the network to find the right weights and biases.
        \item\label{item:solve2} Validate model, i.e. assess model performance by applying it on the test data.
        \item\label{item:solve3} Adjust hyperparameters, and if necessary review the network architecture.
    \end{enumerate}



    \subsubsection{Activation functions}\label{sec:activation_function}

    
    \begin{equation}\label{eq:sigmoid}
        \sigma(\xi) = \frac{1}{1+e^{-\xi}} = 1- \sigma(-\xi)
    \end{equation}

    \begin{equation}\label{eq:tanh}
        \tanh(\xi) = \frac{e^{2\xi}-1}{e^{2\xi}+1} = 2\sigma(2\xi) -1
    \end{equation}

    \begin{equation}\label{eq:relu}
        \mathrm{ReLU}(\xi) = \max(0,\xi) = \begin{cases}
            \xi,\quad &\mathrm{if } \xi >0Â \\
            0,\quad &\mathrm{else}
        \end{cases}
    \end{equation}

    \begin{equation}\label{eq:softmax}
        \zeta(\svec{\xi})_j = \frac{e^{\xi_j}}{\sum\nolimits_{i=1}^{N}e^{\xi_i}}, \quad \svec{\xi} \in \RR[N]
    \end{equation}





    \subsubsection{Back propagation}\label{sec:back_propagation}


    The information in an FFNN accepting input $\vec{x}$ to produce output $\hat{y}$ \checkthis{(check consistency!)} is flowing \textit{forward} \citep{Goodfellow2016}, hence the name. The initial information from $\vec{x}$ propagates through the hidden layers resulting in the production of $\hat{y}$. This information flow is called forward propagation. During training, we let forward propagation yield a cost, $\mathcal{L}(\svec{\theta})$. We may reverse this process, i.e. let $\mathcal{L}(\svec{\theta})$ provide infomation that propagates backwards through the network in order to compute the gradient, \checkthis{$\nabla_{\!\theta}\mathcal{L}(\svec{\theta})$}, corresponding to an algorithm called back-propagation.


\subsection{Classification}\label{sec:classification}

\subsection{Logistic regression}\label{sec:logistic_regression}


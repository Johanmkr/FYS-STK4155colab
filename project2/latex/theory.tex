\section{Theory}\label{sec:theory}

In linear regression we have the famous assumption that 

\begin{align}
    y_i = f(\vec{x}_i) + \epsilon_i \simeq \vec{x}_i\TT \svec{\beta} + \epsilon_i,
\end{align}

where $f$ is a continous funtion of $\vec{x}$. \checkthis{If we now were to allow said function to represent discrete outputs, we would benefit from moving on to \textit{logsitic} regression.} \comment{Maybe move to intro?}

\fillertext[smooth transition to steepest descent]

\subsection{Stochastic gradient descent (SGD)}\label{sec:stochastic_gradient_descent}
    Gradient descent \citep{mhjensen} \fillertext[aim is to find beta etc]

    The result is a flexible way to locate the minima of \checkthis{any} cost function $\mathcal{L}(\svec{\theta})$. The ordinary least squares and Ridge schemes of linear regression that we discussed in \href{https://github.com/Johanmkr/FYS-STK4155colab/tree/main/project1}{project 1}, are then implemented by using the cost functions $\mathcal{L}^\mathrm{OLS}(\svec{\theta})$ and $\mathcal{L}^\mathrm{Ridge}(\svec{\theta})$ respectively, given by the following expressions:

    \begin{subequations}\label{eq:linear_regression_cost_functions}
        \begin{align}
            \mathcal{L}^\mathrm{OLS}(\svec{\theta}) &= \norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2 \label{eq:ols_cost_function}\\
            \mathcal{L}^\mathrm{Ridge}(\svec{\theta})&=\norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2  -  \lambda\norm{\svec{\theta}}_2^2 \label{eq:ridge_cost_function}
        \end{align}
    \end{subequations}

    \comment{Explain different parts}

    \subsubsection{Plain gradient descent (GD)}\label{sec:plain_gradient_descent}
        The most basic concept is that of steepest descent. In order to find a minimum of a function $f(\vec{x})$ (allow multivariability of $\vec{x}$), we follow the steepest descent of that function, i.e. the direction of the negative gradient $-\nabla f(\vec{x})$. We thus have an iterative scheme to find minima:
        \begin{align}\label{eq:steepest_descent}
            \vec{x}_{k+1} = \vec{x}_k - \eta_k\nabla f(\vec{x}_k),
        \end{align}
        where $\eta_k$ may be referred to as the step length or learning rate, which \fillertext

        What we would like to minimise is the cost function $C(\svec{\beta})$ which is a function of the parameters $\svec{\beta}$ which we are trying to estimate. This means that \Eq{steepest_descent} translates into:
        \begin{align}\label{eq:steepest_descent_cost_function}
            \mathcal{A}_k &\equiv \nabla_{\!\theta} \mathcal{L}(\svec{\theta}_k) \nonumber \\
            \vec{v}_k &= -\eta_k\mathcal{A}_k \nonumber \\
            \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k
        \end{align}
        Where we define $\mathcal{A}_k$ to be the direction and magnitude of the steepest ascent in parameter space. 
        For a sufficiently small $\eta_k$, this method will converge to a minimum of $\svec{\beta}$. (However, since we may not know the nature of $\svec{\beta}$ ,there is a risk that this is just a local and not a global minimum. The steepest descent method in \Eq{steepest_descent_cost_function} is a deterministic method, which means we may get stuck in a local minimum. To avoid this we introduce stochastic gradient descent.) 

    \subsubsection{Momentum}\label{sec:momentum}
        From \Eq{steepest_descent_cost_function} we have that the movement in parameter space is given by $\vec{v}_k$ (the negative of which), which describes the direction and magnitude of the steepest ascent in parameter space. Sometimes we might want to move larger distances in one step. This can be achieved by introduction momentum, where we add an addition term to $\vec{v}_k$:
        \begin{align}
            \vec{v}_k &= \gamma\vec{v}_{k-1} - \eta_k\mathcal{A}_k \nonumber \\
            \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k
        \end{align}
        where $\gamma$ is a momentum parameter and $\eta_k$ is the same learning parameter as before. The basic idea is that this with this method we "overshoot" the descending step length in the direction of the previous step, with a magnitude that is controlled by $\gamma$. By doing this, we may reach the desired minimum with fewer iterations. 

        \comment{ADD NAG}
        \begin{align}\label{eq:nag}
            \mathcal{A}_k \to \mathcal{A}_k(\svec{\theta}_k + \gamma\vec{v}_{k-1}) \implies \mathcal{A}_k= \nabla_{\!\theta}\mathcal{L}(\svec{\theta}_k + \gamma\vec{v}_{k-1})
        \end{align}

    \subsubsection{Stochasticity}\label{sec:stochasticity}
        There are several weaknesses to the plain gradient descent, perhaps the largest is the computational expense of on large datasets and its sensitivity of initial conditions and learning rates. If $\svec{\theta}$ have numerous local minima, we will find one minimum only per set of initial conditions, and we have no good way of saying whether this minimum is global or not. One way of overcoming this is by adding stochasticity to the gradient descent algorithm. 

        The main idea is that we have $n$ data points, that we divide into $M$ minibatches (subsets), meaning that we have $n/M$ data points in each minibatch, denoted $B_j$ for $j\in[1,2,\dots,n/M]$. We also recognize that we may write the total cost function as a sum over all data points $\vec{x}_i$ for $i\in[1,n]$, and thus approximate the gradient of the cost function by only summing over the data points in a minibatch picked at random:
        \begin{align}
            \mathcal{L}(\svec{\theta}) &= \sum_{i=1}^nl_i(\vec{x}_i, \svec{\theta}) \nonumber \\
            \nabla_{\!\theta} \mathcal{L}(\svec{\theta}) &= \sum_{i=1}^n \nabla_{\!\theta}l_i(\vec{x}_i,\svec{\theta}) \nonumber \\
            \mathcal{A}^j_k &= \sum_{i\in B_j}\nabla_{\!\theta} l_i(\vec{x}_i, \svec{\theta}_k)
        \end{align}
    \subsubsection{Tuning}\label{sec:tuning}
        \comment{Something about hyper parameters $\lambda$ and $\eta$}

\subsection{Neural Network (NN)}\label{sec:neural_network}

    \subsubsection{Basics}\label{sec:basics}

    \subsubsection{Activation functions}\label{sec:activation_function}

    \subsubsection{Back propagation}\label{sec:back_propagation}

\subsection{Classification}\label{sec:classification}

\subsection{Logistic regression}\label{sec:logistic_regression}


\section{Theory}\label{sec:theory}

\subsection{Gradient descent}\label{sec:gradient_descent}
Gradient descent \citep{mhjensen}
The most basic concept is that of steepest descent. In order to find a minimum of a function $f(\vec{x})$ (allow multivariability of $\vec{x}$), we follow the steepest descent of that function, i.e. the direction of the negative gradient $-\nabla f(\vec{x})$. We thus have an iterative scheme to find minima:
\begin{align}\label{eq:steepest_descent}
    \vec{x}_{k+1} = \vec{x}_k - \gamma_k\nabla f(\vec{x}_k),
\end{align}
where $\gamma_k$ may be referred to as the step length or learning rate, which

\subsubsection{Plain Gradient Descent}\label{sec:plain_gradient_descent}

\subsubsection{Stochastic Gradient Descent}\label{sec:stochastic_gradient_descent}
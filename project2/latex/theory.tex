\section{Theory}\label{sec:theory}

In linear regression we have the famous assumption that 

\begin{align}
    y_i = f(\vec{x}_i) + \epsilon_i \simeq \vec{x}_i\TT \svec{\beta} + \epsilon_i,
\end{align}

where $f$ is a continous funtion of $\vec{x}$. \checkthis{If we now were to allow said function to represent discrete outputs, we would benefit from moving on to \textit{logsitic} regression.} \comment{Maybe move to intro?}

\fillertext[smooth transition to steepest descent]

\subsection{Stochastic gradient descent (SGD)}\label{sec:stochastic_gradient_descent}
    Gradient descent \citep{mhjensen} \fillertext[aim is to find beta etc]

    The result is a flexible way to locate the minima of \checkthis{any} cost function $\mathcal{L}(\svec{\theta})$. The ordinary least squares and Ridge schemes of linear regression that we discussed in \href{https://github.com/Johanmkr/FYS-STK4155colab/tree/main/project1}{project 1}, are then implemented by using the cost functions $\mathcal{L}^\mathrm{OLS}(\svec{\theta})$ and $\mathcal{L}^\mathrm{Ridge}(\svec{\theta})$ respectively, given by the following expressions:

    % \begin{subequations}\label{eq:linear_regression_cost_functions}
    %     \begin{align}
    %         \mathcal{L}^\mathrm{OLS}(\svec{\theta}) &= \norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2 \label{eq:ols_cost_function}\\
    %         \mathcal{L}^\mathrm{Ridge}(\svec{\theta})&=\norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2  -  \lambda\norm{\svec{\theta}}_2^2 \label{eq:ridge_cost_function}
    %     \end{align}
    % \end{subequations}

    \begin{subequations}\label{eq:linear_regression_cost_functions}
        \begin{align}
            \mathcal{L}^\mathrm{OLS}(\svec{\theta}) &=  \norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2 \label{eq:ols_cost_function}\\
            \mathcal{L}^\mathrm{Ridge}(\svec{\theta})&=\norm{\vec{f}(X; \svec{\theta} ) - \vec{y}}_2^2  -  \lambda\norm{\svec{\theta}}_2^2 \label{eq:ridge_cost_function}
        \end{align}
    \end{subequations}

    \comment{Explain different parts}
 
    \subsubsection{Plain gradient descent (GD)}\label{sec:plain_gradient_descent}
        The most basic concept is that of steepest descent. In order to find a minimum of a function $\rho(\svec{\xi})$ (allow multivariability of $\svec{\xi}$), we follow the steepest descent of that function, i.e. the direction of the negative gradient $-\nabla_{\!\xi} \rho(\svec{\xi})$. We thus have an iterative scheme to find minima:
        \begin{align}\label{eq:steepest_descent}
            \svec{\xi}_{k+1} = \svec{\xi}_k - \eta_k\nabla_{\!\xi} \rho(\svec{\xi}_k),
        \end{align}
        where the learning rate $\eta_k$ goes as
        \begin{equation}\label{eq:learning_schedule}
            \eta_k = \begin{cases}
                \big(1-\frac{k}{\tau}\big)\eta_0 + \frac{k}{\tau}\eta_\tau,\quad & 0 \leq k \leq \tau \\
                \eta_\tau, & k>\tau
            \end{cases}
        \end{equation}
        for an initial rate $\eta_0$ and lower limit $\eta_\tau \sim 0.01 \eta_0$.

        % A is step size
        \fillertext

        What we would like to minimise is the cost function $\mathcal{L}(\svec{\theta})$ which is a function of the parameters $\svec{\theta}$ which we are trying to estimate. If we define $\mathcal{A}_k \equiv \nabla_{\!\theta} \mathcal{L}(\svec{\theta}_k)$ to be the direction and magnitude of the steepest ascent in parameter space, eq. \eqref{eq:steepest_descent} reads 
        \begin{equation}\label{eq:update_rule_general}
            \begin{split}
                \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k\,;  \\
                &\quad \vec{v}_k = -\eta_k\mathcal{A}_k,
            \end{split}
        \end{equation}
        for substitutions $\rho\to\mathcal{L}$ and $\svec{\xi}\to \svec{\theta}$. For a sufficiently small $\eta_k$, this method will converge to a minimum of $\svec{\theta}$. (However, since we may not know the nature of $\svec{\theta}$, there is a risk that this is just a local and not a global minimum. The steepest descent method in eq. \eqref{eq:update_rule_general} is a deterministic method, which means we may get stuck in a local minimum. To avoid this we introduce stochastic gradient descent.) 

    \subsubsection{Momentum}\label{sec:momentum}
        From eq. \eqref{eq:update_rule_general} we have that the movement in parameter space is given by $\vec{v}_k$ (the negative of which), which describes the direction and magnitude of the steepest ascent in parameter space. Sometimes we might want to move larger distances in one step. This can be achieved by introduction momentum: We add an addition term to $\vec{v}_k$ which lets us rewrite eq. \eqref{eq:update_rule_general} as
        \begin{equation}\label{eq:momentum_GD_algorithm}
            \begin{split}
                \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k\, ; \\
                &\quad \vec{v}_k =\gamma\vec{v}_{k-1} - \eta_k\mathcal{A}_k,
            \end{split}
        \end{equation}
        where $\gamma$ is a momentum parameter and $\eta_k$ is the same learning parameter as before. The basic idea is that this with this method we "overshoot" the descending step length in the direction of the previous step, with a magnitude that is controlled by $\gamma$. By doing this, we may reach the desired minimum with fewer iterations. 

        There are several modifications we can do to optimise this algorithm. In the Nesterov momentum algorithm (NAG) one applies the following adjustment to the gradient in eq. \eqref{eq:momentum_GD_algorithm}:
        \begin{equation}\label{eq:nag}
            \begin{split}
                \mathcal{A}_k &\to\nabla_{\!\tilde{\theta}}\mathcal{L}(\tilde{\svec{\theta}}_k) \, ;\\ 
                & \quad \tilde{\svec{\theta}}_k = \svec{\theta}_k + \gamma\vec{v}_{k-1} \,Â ;
            \end{split}
        \end{equation}

        This is analagous to the adjustment needed to go from forward Euler to Euler-Cromer as numerical intregration method.

        \fillertext

        
        


        

    \subsubsection{Stochasticity}\label{sec:stochasticity}
        There are several weaknesses to the plain gradient descent, perhaps the largest is the computational expense of on large datasets and its sensitivity of initial conditions and learning rates. If $\mathcal{L}(\svec{\theta})$ has numerous local minima, we will find one minimum only per set of initial conditions, and we have no good way of saying whether this minimum is global or not. One way of overcoming this is by adding stochasticity to the gradient descent algorithm. 

        The main idea is that we have $n$ data points, that we divide into $m=n/M$\footnote{We of course need to make sure this is an integer. \comment{Explain?}} minibatches (subsets), meaning that we have $M$ data points in each minibatch, denoted $\mathcal{B}_j$ for $j\in\{1,2,\dots,m\}$, s.t. $\bigcup_{j=1}^m \mathcal{B}_j = \bigcap_{j=1}^m \mathcal{B}_j = \mathcal{D}$. We also recognise that we may write the total cost function as a sum over all data points $\vec{x}^{(i)}$ for $i\in[1,n]$, 
        \begin{equation}
            \mathcal{L}(\svec{\theta}) =\sum_{(\vec{x}, y)\in \mathcal{D}} l\big(f(\vec{x};\,\svec{\theta}), y \big) = \sum_{i=1}^n l_i(\svec{\theta}),
        \end{equation}
        where $l_i = l\big(f(\vec{x}^{(i)};\svec{\theta}); \, y^{(i)}\big)$, and thus its gradient reads
        \begin{equation}
            \mathcal{A} = \nabla_{\!\theta} \mathcal{L}(\svec{\theta}) = \sum_{i=1}^n \nabla_{\!\theta}l_i(\svec{\theta}).
        \end{equation}
        Now we may approximate the gradient of the cost function by only summing over the data points in a minibatch picked at random:
        \begin{equation}\label{eq:sgd_gradient}
            \begin{split}
            \mathcal{A}_k &= \sum_{j=1}^{m}\mathcal{A}_k^j\, ; \\
            \mathcal{A}_k &\to \mathcal{A}^j_k = \sum_{i:\vec{x}^{(i)} \in\mathcal{B}_j }\nabla_{\!\theta} l_i(\svec{\theta}_k).
            \end{split}
        \end{equation}
        The estimate $\mathcal{A}_k^j \approx \mathcal{A}_k$ can be used in our algorithm to ensure stochasticity and relieve computational pressure.
    
    \subsubsection{Optimising the learning rate}\label{sec:tuning}
    
    We can mitigate parts of the struggle with hyperparameter adjustment by using an algorithm with adaptive learning rates. We present a few such schemes in short below, that is different ways of calculating $\vec{v}$ in eq. \eqref{eq:update_rule_general}. All require an original learning rate $\eta$ and a small number $\epsilon$ for numerical stability. 

    \begin{enumerate}[leftmargin=0pt,labelwidth=!,labelsep=.05em]
        \item[]\textbf{AdaGrad} \citep[algorithm 8.4]{Goodfellow2016}, from "adaptive gradient (algorithm)", adapts $\eta$ individually to the components of $\svec{\theta}$, scaling them as $\eta \to \eta' \sim \nicefrac{\eta}{\nabla_{\! \theta} \mathcal{L}}$ (mind the simplification). This method is famously a \rephrase{trouper} in convex settings, but has the unfortunate habit of prematurely deciding on the best model in a nonconvex setting. By default: $\epsilon=10^{-7}$.
        \item[]\textbf{RMSProp} \citep[algorithm 8.5]{Goodfellow2016}, from "root mean squared propagation", is a modification to the AdaGrad algorithm. Where AdaGrad performs good, this one learns slow in comparison. However, RMSProp outperforms AdaGrad in nonconvex situations. This improvement introduces an additional hyperparameter $\rho$. By default: $\rho = 0.9$ and $\epsilon=10^{-7}$.
        \item[]\textbf{Adam} \citep[algorithm 8.7]{Goodfellow2016}, from "adaptive moments", \fillertext We need two hyperparameters $\rho_1, \rho_1$ for this optimisation scheme. By default: $\rho_1 = 0.9$, $\rho_2=0.999$ and $\epsilon=10^{-8}$.
    \end{enumerate}

    
    
\subsection{Neural Network (NN)}\label{sec:neural_network}

    \subsubsection{Basics}\label{sec:basics}

    A feedforward NN (FFNN) is typically built by composing together several functions into a chain of function. Associated with this model is a directed acyclic graph (DAG) describing the explicit structure. The depth of the model is determined by the length of the abovementioned chain. Each function represents a layer in the network. The final layer of an FFNN is the output layer, and the layers between the input (prior to the first) and the output layer are called hidden layers. \citep{Goodfellow2016}

    The structure of such a chain-based architecture is described by the $L-1$ hidden layers $\vec{h}^l \in \RR[N_l],\,l=1,2, \dots, L-1$, given by
    \begin{subequations}\label{eq:nn_layers}
        \begin{align} 
            \vec{h}^0 &= \vec{x}^{(i)}\,; \\% dunno
            \vec{h}^1 &= g_1 \big((W^{0\to 1})\TT \vec{h}^0 + \vec{b}^1\big)\,; \\
            \vec{h}^2 &= g_2 \big( (W^{1\to 2})\TT \vec{h}^1 + \vec{b}^2\big)\,; \\
            &\vdots \nonumber \\
            \vec{h}^{L} &= g_L \big((W^{{L\!-\!1\to L}})\TT \vec{h}^{L\!-\! 1} + \vec{b}^L\big)\,;             
        \end{align}
    \end{subequations}
    where we defined $\vec{h}^0$ and $\vec{h}^L$ to be the input and output layer, respectively. 

    The matrix of weights $W^{l-\! 1\to 1} \in \RR[N_{l-\!1} \cross N_l]$ applies weights and dimension corrections to the previous layer $\vec{h}^{l-\! 1}\in \RR[N_{l-\! 1}]$ so that the activation function $g_l \,:\, \RR[N_l] \to \RR[N_l]$ can accept the input. \rephrase{Rephrase?} The bias $\vec{b}^l \in \RR[N_l]$ \wtf

   



    \subsubsection{Activation functions}\label{sec:activation_function}

    
    % \begin{equation}\label{eq:sigmoid}
    %     \sigma(\xi) = \frac{1}{1+e^{-\xi}} = 1- \sigma(-\xi)
    % \end{equation}

    % \begin{equation}\label{eq:tanh}
    %     \tanh(\xi) = \frac{e^{2\xi}-1}{e^{2\xi}+1} = 2\sigma(2\xi) -1
    % \end{equation}

    % \begin{equation}\label{eq:relu}
    %     \mathrm{ReLU}(\xi) = \max(0,\xi) = \begin{cases}
    %         \xi,\quad &\mathrm{if }\, \xi >0 \\
    %         0,\quad &\mathrm{else}
    %     \end{cases}
    % \end{equation}

    % \begin{equation}\label{eq:leaky_relu}
    %     \mathrm{lReLU}(\xi)  = \begin{cases}
    %         \xi,\quad &\mathrm{if }\, \xi >0 \\
    %         0.01\xi,\quad &\mathrm{else}
    %     \end{cases}
    % \end{equation}

    % \begin{equation}\label{eq:softmax}
    %     \zeta(\svec{\xi})_j = \frac{e^{\xi_j}}{\sum\nolimits_{i=1}^{N}e^{\xi_i}}, \quad \svec{\xi} \in \RR[N]
    % \end{equation}
    A layer $\vec{h}^l$ have an associated activation $\vec{a}^l \in \RR[N_l]$ containing the weights \comment{Is this general?} and biases linked with its neurons. The activation is passed as argument to the activation function $g_l$ whose job is to perform the the affine transformation from one layer to another in an NN. In eq. \eqref{eq:nn_layers} the activation is $\vec{a}^l = (W^{l-\! 1\to l})\TT\vec{h}^{l -\!1} + \vec{b}^l$, which is valid for $l= 1, 2,\dots L$. Note that the weight matrix $W^{l-\! 1\to l}$ is associated with both the current and previous layer. We can rewrite the formula in eq. \eqref{eq:nn_layers} as the more compact expression
    \feltcute{
    \begin{equation}
        \begin{split}
            \vec{h}^0 &= \vec{x} \,,\quad \vec{h}^l =  g_l(\vec{a}^l)\,, \, \, l=1,2, \dots L \,; \\
            &\quad \vec{a}^l = W^{l\leftarrow l-\!1}\vec{h}^{l -\!1} + \vec{b}^l \,;
        \end{split}
    \end{equation}
    where $W^{l\leftarrow l-\!1} \equiv (W^{l-\! 1\to l})\TT$.}
    
    The output is \checkthis{$\hat{\vec{y}}^{(i)} = \vec{h}^L \in \RR[N_L]$}.


    \begin{subequations}\label{eq:activation_functions}
        \begin{align}
            &\sigma(\svec{\xi}) = \frac{1}{1+e^{-\svec{\xi}}} = 1- \sigma(-\svec{\xi})\label{eq:sigmoid}\\
            &\tanh(\svec{\xi}) = \frac{e^{2\svec{\xi}}-1}{e^{2\svec{\xi}}+1} = 2\sigma(2\svec{\xi}) -1\label{eq:tanh} \\
            &\mathrm{ReLU}(\svec{\xi}) = \max(0,\svec{\xi}) = \begin{cases}
                \svec{\xi},\quad &\svec{\xi} >0 \\
                0,\quad &\svec{\xi} \leq 0
            \end{cases} \label{eq:relu}\\
            &\mathrm{ReLU}^*(\svec{\xi})  = \begin{cases}
                \svec{\xi},\quad &\svec{\xi} >0 \\
                0.01\svec{\xi},\quad &\svec{\xi} \leq 0
            \end{cases} \label{eq:leaky_relu}
        \end{align}
    \end{subequations}

    Assuming some $\svec{\xi} \in \RR[K]$, the set of expressions \eqref{eq:activation_functions} show some well-known activation functions $\RR[K] \to \RR[K]$. To translate into NN-components, set $K\to N$, $\svec{\xi} \to \vec{a}$ and e.g. $g = \tanh$. 
    The oldest and probably most famous is the slow-learning sigmoid function $\sigma$ in eq. \eqref{eq:sigmoid}. The hyperbolic tangent in eq. \eqref{eq:tanh} is closely related to the sigmoid, and is typically performing better \citep{Goodfellow2016}. The ReLU (eq. \eqref{eq:relu}) or leaky ReLU (eq. \eqref{eq:leaky_relu}) activation function provides output of the type that is easy to interpret as it resembles the linear unit. ReLU typically learns fast, but has the the unfortunate habit of killing neurons. That is to say, some neurons are deactivated for any input. The leaky ReLU can omit this issue somewhat, but there hatch is a perfomance reduction.

    





    \subsubsection{Back propagation}\label{sec:back_propagation}


    The information in an FFNN accepting input $\vec{x}$ to produce output $\hat{y}$ \checkthis{(check consistency!)} is flowing \textit{forward} \citep{Goodfellow2016}, hence the name. The initial information from $\vec{x}$ propagates through the hidden layers resulting in the production of $\hat{y}$. This information flow is called forward propagation. During training, we let forward propagation yield a cost, $\mathcal{L}(\svec{\theta})$. We may reverse this process, i.e. let $\mathcal{L}(\svec{\theta})$ provide infomation that propagates backwards through the network in order to compute the gradient, \checkthis{$\nabla_{\!\theta}\mathcal{L}(\svec{\theta})$}, corresponding to an algorithm called back-propagation.


\subsection{Classification}\label{sec:classification}

\subsection{Logistic regression}\label{sec:logistic_regression}

The standard logistic function $p:\, \RR \to (0,1)$ may be written as 

\begin{equation}\label{eq:logistic_function}
    p(\xi) = \frac{1}{1+e^{-(\beta_0 + \beta_1\xi)}},
\end{equation}

and is indeed the sigmoid function in eq. \eqref{eq:sigmoid} if we substitute $\xi \to \beta_0 + \beta_1 \xi$. 
% beta or theta?
\fillertext

% \begin{equation}\label{eq:logistic_cost_function}
%     \mathcal{L}(\svec{\beta}) = - \sum_{i=1}^n \Big[ y^{(i)} \big(\beta_0+ \beta_1 x^{(i)}\big) - \log{\Big(1+e^{\beta_0 + \beta_1 x^{(i)}}\Big)} \Big]
% \end{equation}


\comment{This is rubbish}


\begin{equation}\label{eq:logistic_cost_function}
    \mathcal{L}(\svec{\theta}) = -  \frac{1}{n}\sum_{i=1}^n \Big[ y^{(i)} \big(\theta_0 + (X\svec{\theta})^{(i)}\big) - \log{\Big(1+e^{\theta_0 + \svec{\theta}\TT \vec{x}^{(i)}}\Big)} \Big]
\end{equation}

In terms of weights and biases ($ (\theta_0,  \svec{\theta} ) \to (\vec{b}, W)$),

\begin{equation}\label{eq:logistic_cost_function_NN}
    \mathcal{L}(\vec{b}, W) = -  \frac{1}{n}\sum_{i=1}^n \Big[ y^{(i)} \big(\vec{b} + W\TT \vec{h}^{l}\big) - \log{\Big(1+e^{\theta_0 + \svec{\theta}\TT \vec{x}^{(i)}}\Big)} \Big]
\end{equation}

\begin{equation}\label{eq:logistic_cost_function_NN2}
    \mathcal{L}(l) = -  \frac{1}{N_l} \Big[ (\vec{h}^{l})\TT\vec{a}^{l} - \log{\big(1+e^{\vec{a}^{l}}\big)} \Big]
\end{equation}

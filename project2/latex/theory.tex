\section{Theory}\label{sec:theory}

Linear regression assumes a linear relationship between a set of $p$ features $\vec{x}\in\RR[p]$ and an observed value $y\in\RR$. We assume there to exist a \textit{continous} function of the input $\vec{x}$ giving the output $\hat{y}$. The coefficients $\svec{\theta}\in\RR[p]$ that determine said function can be estimated using a variety of methods, as discussed in \projectOne. In any case, the aim is to minimise some loss function $\mathcal{L}(\svec{\theta};\,\hat{y}, y)$ with respect to this parameter vector ($\svec{\theta}$) describing what we sacrifice by using this exact model. 

What if the function we want to fit is \textit{discontinous}? We consider the binary situation where the observed $y$ only takes one of two discrete values; 0 or 1. Logistic regression proposes a model where the output $\hat{y}$ is obtained from  a probability distribution and subsequently a befitting total loss function that can be minimised with respect to a set of parameters $\svec{\theta}$. Now, instead of using the method-specific regression algorithms for finding the optimal $\svec{\theta}$, we can change modus operandi and focus solely on the minimisation of some objective function (e.g. a loss function such as the \checkthis{MSE}). For this purpose, we may use the very powerful procedure of steepest descent.

Where the actual (physical) relationship between some dependent and independent variable is not of \rephrase{utmost importance}, and the main aim is to predict the outcome given some setting, supervised learning problems may also be solved using neural networks. 

\par{*} NN - no parameter vector - allow multivariable y



\subsection{Stochastic gradient descent (SGD)}\label{sec:stochastic_gradient_descent}
    SGD and its subvariants are frequently used optimisation algorithms in machine learning \citep{Goodfellow2016}. The more basic algorithm known as gradient descent (GD) is technically a specific case of SGD\footnote{The case with number of minibatches $m=1$.} follows the gradient of some objective function $J$ downhill in some parameter space. The effect of introducing stochasticity is \rephrase{discussed} in section \ref{sec:stochasticity}. The result is a flexible way to locate the minima of \checkthis{any} $J$, exactly what we wished for. The ordinary least squares and Ridge schemes of linear regression that we discussed in \projectOne, are then implemented by using the mean squared error (MSE) function with an \lnorm[2] regularisation term,
    \begin{equation}\label{eq:linear_regression_cost_function}
        \mathcal{L}(\svec{\theta}) = \frac{1}{2n} \sum_{i=1}^n (\hat{y}^{(i)}-y^{(i)})^2 + \frac{\lambda}{2p} \sum_{j=1}^{p} \theta_j^2. 
    \end{equation}
    $\lambda$ is the penalty term set to zero for OLS and a small positive value for Ridge regression. The output $\hat{y}^{(i)}$ is resulting from some function evaluated at $\vec{x}^{(i)}$, which for our purposes reads $\hat{y}^{(i)} = [\vec{x}^{(i)}]\TT \svec{\theta}$.

    \fillertext

 
    \subsubsection{Plain gradient descent}\label{sec:plain_gradient_descent}
        The most basic concept is that of steepest descent. In order to find a minimum of a function $J=J(\svec{\xi})$, we follow the steepest descent of that function, i.e. the direction of the negative gradient $-\nabla_{\!\xi} \rho(\svec{\xi})$. We thus have the iterative scheme to find minima,
        \begin{align}\label{eq:steepest_descent}
            \svec{\xi}_{k+1} = \svec{\xi}_k - \eta_k\nabla_{\!\xi} J(\svec{\xi}_k),
        \end{align}
        where the learning rate $\eta_k$ may follow a schedule in $k$ or stay constant. In the following, we consider a constant global\footnote{Emphesising "global" here to distinguish from the \textit{actual} rate of learning (or step size) which may depend on the specific update rule we choose.} learning rate $\eta_k=\eta$.
        
        What we would like to minimise is the cost function $\mathcal{L}(\svec{\theta})$ which is a function of the parameters $\svec{\theta}$ which we are trying to estimate. If we define $\mathcal{A}_k \equiv \nabla_{\!\theta} \mathcal{L}(\svec{\theta}_k)$ to be the direction and magnitude of the steepest ascent in parameter space, eq. \eqref{eq:steepest_descent} reads 
        \begin{equation}\label{eq:update_rule_general}
            \begin{split}
                \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k\,;  \\
                &\quad \vec{v}_k = -\eta_k\mathcal{A}_k,
            \end{split}
        \end{equation}
        for substitutions $\rho\to\mathcal{L}$ and $\svec{\xi}\to \svec{\theta}$. For a sufficiently small $\eta_k$, this method will converge to a minimum of $\svec{\theta}$. (However, since we may not know the nature of $\svec{\theta}$, there is a risk that this is just a local and not a global minimum. The steepest descent method in eq. \eqref{eq:update_rule_general} is a deterministic method, which means we may get stuck in a local minimum. To avoid this we introduce stochastic gradient descent.) 

    \subsubsection{Momentum}\label{sec:momentum}
        From eq. \eqref{eq:update_rule_general} we have that the movement in parameter space is given by $\vec{v}_k$ (the negative of which), which describes the direction and magnitude of the steepest ascent in parameter space. Sometimes we might want to move larger distances in one step. This can be achieved by introduction momentum: We add an addition term to $\vec{v}_k$ which lets us rewrite eq. \eqref{eq:update_rule_general} as
        \begin{equation}\label{eq:momentum_GD_algorithm}
            \begin{split}
                \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k\, ; \\
                &\quad \vec{v}_k =\gamma\vec{v}_{k-1} - \eta_k\mathcal{A}_k,
            \end{split}
        \end{equation}
        where $\gamma$ is a momentum parameter and $\eta_k$ is the same learning parameter as before. The basic idea is that this with this method we "overshoot" the descending step length in the direction of the previous step, with a magnitude that is controlled by $\gamma$. By doing this, we may reach the desired minimum with fewer iterations. 

        There are several modifications we can do to optimise this algorithm. In the Nesterov momentum algorithm (NAG) one applies the following adjustment to the gradient in eq. \eqref{eq:momentum_GD_algorithm}:
        \begin{equation}\label{eq:nag}
            \begin{split}
                \mathcal{A}_k &\to\nabla_{\!\tilde{\theta}}\mathcal{L}(\tilde{\svec{\theta}}_k) \, ;\\ 
                & \quad \tilde{\svec{\theta}}_k = \svec{\theta}_k + \gamma\vec{v}_{k-1} \,Â ;
            \end{split}
        \end{equation}

        This is analagous to the adjustment needed to go from forward Euler to Euler-Cromer as numerical intregration method.

        \fillertext

        
        


        

    \subsubsection{Stochasticity}\label{sec:stochasticity}
        There are several weaknesses to the plain gradient descent, perhaps the largest is the computational expense of on large datasets and its sensitivity of initial conditions and learning rates. If $\mathcal{L}(\svec{\theta})$ has numerous local minima, we will find one minimum only per set of initial conditions, and we have no good way of saying whether this minimum is global or not. One way of overcoming this is by adding stochasticity to the gradient descent algorithm. 

        The main idea is that we have $n$ data points, that we divide into $m=n/M$\footnote{We of course need to make sure this is an integer. \comment{Explain?}} minibatches (subsets), meaning that we have $M$ data points in each minibatch, denoted $\mathcal{B}_j$ for $j\in\{1,2,\dots,m\}$, s.t. $\bigcup_{j=1}^m \mathcal{B}_j = \bigcap_{j=1}^m \mathcal{B}_j = \mathcal{D}$. We also recognise that we may write the total cost function as a sum over all data points $\vec{x}^{(i)}$ for $i\in[1,n]$, 
        \begin{equation}
            \mathcal{L}(\svec{\theta}) =\sum_{(\vec{x}, y)\in \mathcal{D}} l\big(f(\vec{x};\,\svec{\theta}), y \big) = \sum_{i=1}^n l_i(\svec{\theta}),
        \end{equation}
        where $l_i = l\big(f(\vec{x}^{(i)};\svec{\theta}); \, y^{(i)}\big)$, and thus its gradient reads
        \begin{equation}
            \mathcal{A} = \nabla_{\!\theta} \mathcal{L}(\svec{\theta}) = \sum_{i=1}^n \nabla_{\!\theta}l_i(\svec{\theta}).
        \end{equation}
        Now we may approximate the gradient of the cost function by only summing over the data points in a minibatch picked at random:
        \begin{equation}\label{eq:sgd_gradient}
            \begin{split}
            \mathcal{A}_k &= \sum_{j=1}^{m}\mathcal{A}_k^j\, ; \\
            \mathcal{A}_k &\to \mathcal{A}^j_k = \sum_{i:\vec{x}^{(i)} \in\mathcal{B}_j }\nabla_{\!\theta} l_i(\svec{\theta}_k).
            \end{split}
        \end{equation}
        The estimate $\mathcal{A}_k^j \approx \mathcal{A}_k$ can be used in our algorithm to ensure stochasticity and relieve computational pressure.
    
    \subsubsection{Optimising the learning rate}\label{sec:tuning}
    
    We can mitigate parts of the struggle with hyperparameter adjustment by using an algorithm with adaptive learning rates. We present a few such schemes in short below, that is different ways of calculating $\vec{v}$ in eq. \eqref{eq:update_rule_general}. All require an original learning rate $\eta$ and a small number $\epsilon$ for numerical stability. 

    \begin{enumerate}[leftmargin=0pt,labelwidth=!,labelsep=.05em]
        \item[]\textbf{AdaGrad} \citep[algorithm 8.4]{Goodfellow2016}, from "adaptive gradient (algorithm)", adapts $\eta$ individually to the components of $\svec{\theta}$, scaling them as $\eta \to \eta' \sim \nicefrac{\eta}{\nabla_{\! \theta} \mathcal{L}}$ (mind the simplification). This method is famously a \rephrase{trouper} in convex settings, but has the unfortunate habit of prematurely deciding on the best model in a nonconvex setting. By default: $\epsilon=10^{-7}$.
        \item[]\textbf{RMSProp} \citep[algorithm 8.5]{Goodfellow2016}, from "root mean squared propagation", is a modification to the AdaGrad algorithm. Where AdaGrad performs good, this one learns slow in comparison. However, RMSProp outperforms AdaGrad in nonconvex situations. This improvement introduces an additional hyperparameter $\rho$. By default: $\rho = 0.9$ and $\epsilon=10^{-7}$.
        \item[]\textbf{Adam} \citep[algorithm 8.7]{Goodfellow2016}, from "adaptive moments", \fillertext We need two hyperparameters $\rho_1, \rho_2$ for this optimisation scheme. By default: $\rho_1 = 0.9$, $\rho_2=0.999$ and $\epsilon=10^{-8}$.
    \end{enumerate}

    
    
\subsection{Neural Network (NN)}\label{sec:neural_network}

    \subsubsection{Basics}\label{sec:basics}

    A feedforward NN (FFNN) is typically built by composing together several functions into a chain of function. Associated with this model is a directed acyclic graph (DAG) describing the explicit structure. The depth of the model is determined by the length of the abovementioned chain. Each function represents a layer in the network. The final layer of an FFNN is the output layer, and the layers between the input (prior to the first) and the output layer are called hidden layers. \citep{Goodfellow2016}

    The structure of such a chain-based architecture is described by the $L-1$ hidden layers $\vec{h}^l \in \RR[N_l],\,l=1,2, \dots, L-1$, given by
    \begin{subequations}\label{eq:nn_layers}
        \begin{align} 
            \vec{h}^0 &= \vec{x}^{(i)}\,; \\% dunno
            \vec{h}^1 &= g_1 \big((W^{0\to 1})\TT \vec{h}^0 + \vec{b}^1\big)\,; \\
            \vec{h}^2 &= g_2 \big( (W^{1\to 2})\TT \vec{h}^1 + \vec{b}^2\big)\,; \\
            &\vdots \nonumber \\
            \vec{h}^{L} &= g_L \big((W^{{L\!-\!1\to L}})\TT \vec{h}^{L\!-\! 1} + \vec{b}^L\big)\,;             
        \end{align}
    \end{subequations}
    where we defined $\vec{h}^0$ and $\vec{h}^L$ to be the input and output layer, respectively. 

    The matrix of weights $W^{l-\! 1\to l} \in \RR[N_{l-\!1} \cross N_l]$ applies weights and dimension corrections to the previous layer $\vec{h}^{l-\! 1}\in \RR[N_{l-\! 1}]$ so that the activation function $g_l \,:\, \RR[N_l] \to \RR[N_l]$ can accept the input. The bias $\vec{b}^l \in \RR[N_l]$ may be interpreted as a safety mechanism of the neurons to prevent their layer value to become zero, and is typically set to a small non-zero value \citep{Goodfellow2016}. 

   



    \subsubsection{Activation functions}\label{sec:activation_function}

    
    % \begin{equation}\label{eq:sigmoid}
    %     \sigma(\xi) = \frac{1}{1+e^{-\xi}} = 1- \sigma(-\xi)
    % \end{equation}

    % \begin{equation}\label{eq:tanh}
    %     \tanh(\xi) = \frac{e^{2\xi}-1}{e^{2\xi}+1} = 2\sigma(2\xi) -1
    % \end{equation}

    % \begin{equation}\label{eq:relu}
    %     \mathrm{ReLU}(\xi) = \max(0,\xi) = \begin{cases}
    %         \xi,\quad &\mathrm{if }\, \xi >0 \\
    %         0,\quad &\mathrm{else}
    %     \end{cases}
    % \end{equation}

    % \begin{equation}\label{eq:leaky_relu}
    %     \mathrm{lReLU}(\xi)  = \begin{cases}
    %         \xi,\quad &\mathrm{if }\, \xi >0 \\
    %         0.01\xi,\quad &\mathrm{else}
    %     \end{cases}
    % \end{equation}

    % \begin{equation}\label{eq:softmax}
    %     \zeta(\svec{\xi})_j = \frac{e^{\xi_j}}{\sum\nolimits_{i=1}^{N}e^{\xi_i}}, \quad \svec{\xi} \in \RR[N]
    % \end{equation}
    A layer $\vec{h}^l$ have an associated activation $\vec{a}^l \in \RR[N_l]$ which is a function of the previous layer values ,$h^{l-\!1}$, the weights, $W^{l-\!1\to l}$, and the biases linked with each neuron $b^l$. The activation is passed as argument to the activation function $g_l$ whose job is to perform the affine transformation from one layer to another in a NN. In eq. \eqref{eq:nn_layers} the activation is $\vec{a}^l = (W^{l-\! 1\to l})\TT\vec{h}^{l -\!1} + \vec{b}^l$, which is valid for $l= 1, 2,\dots L$. Note that the weight matrix $W^{l-\! 1\to l}$ is associated with both the current and previous layer. We can rewrite the formula in eq. \eqref{eq:nn_layers} as the more compact expression:
    \begin{equation}
        \begin{split}
            \vec{h}^0 &= \vec{x} \,,\quad \vec{h}^l =  g_l(\vec{a}^l)\,, \, \, l=1,2, \dots L \,; \\
            &\quad \vec{a}^l = W^{l\leftarrow l-\!1}\vec{h}^{l -\!1} + \vec{b}^l \,;
        \end{split}
    \end{equation}
    where $W^{l\leftarrow l-\!1} \equiv (W^{l-\! 1\to l})\TT$. The output is $\hat{\vec{y}}^{(i)} = \vec{h}^L \in \RR[N_L]$.


    \begin{subequations}\label{eq:activation_functions}
        \begin{align}
            &\sigma(\svec{\xi}) = \frac{1}{1+e^{-\svec{\xi}}} = 1- \sigma(-\svec{\xi})\label{eq:sigmoid}\\
            &\tanh(\svec{\xi}) = \frac{e^{2\svec{\xi}}-1}{e^{2\svec{\xi}}+1} = 2\sigma(2\svec{\xi}) -1\label{eq:tanh} \\
            &\mathrm{ReLU}(\svec{\xi}) = \max(0,\svec{\xi}) = \begin{cases}
                \svec{\xi},\quad &\svec{\xi} >0 \\
                0,\quad &\svec{\xi} \leq 0
            \end{cases} \label{eq:relu}\\
            &\mathrm{ReLU}^*(\svec{\xi})  = \begin{cases}
                \svec{\xi},\quad &\svec{\xi} >0 \\
                0.01\svec{\xi},\quad &\svec{\xi} \leq 0
            \end{cases} \label{eq:leaky_relu}
        \end{align}
    \end{subequations}

    Assuming some $\svec{\xi} \in \RR[K]$, the set of expressions \eqref{eq:activation_functions} show some well-known activation functions $\RR[K] \to \RR[K]$. To translate into NN-components, set $K\to N$, $\svec{\xi} \to \vec{a}$ and e.g. $g = \tanh$. 
    The oldest and probably most famous is the slow-learning sigmoid function $\sigma$ in eq. \eqref{eq:sigmoid}. The hyperbolic tangent in eq. \eqref{eq:tanh} is closely related to the sigmoid, and is typically performing better \citep{Goodfellow2016}. The ReLU (eq. \eqref{eq:relu}) or leaky ReLU (eq. \eqref{eq:leaky_relu}) activation function provides output of the type that is easy to interpret as it resembles the linear unit. ReLU typically learns fast, but has the the unfortunate habit of killing neurons. That is to say, some neurons are deactivated for any input. The leaky ReLU can omit this issue somewhat, but the hatch is a perfomance reduction.

    





    \subsubsection{Back propagation}\label{sec:back_propagation}


    The information in an FFNN accepting input $\vec{x}$ to produce output $\vec{\hat{y}}$ is flowing \textit{forward} \citep{Goodfellow2016}, hence the name. The initial information from $\vec{x}$ propagates through the hidden layers resulting in the production of $\vec{\hat{y}}$ which is the output of the final layer. This information flow is called forward propagation or forward pass. Training the network (tuning the weights and biases) consist of running forward propagation and compare the resultant output $\vec{\hat{y}}$ with the desired output $\vec{y}$, i.e. evaluate the loss function, $\mathcal{L}(\svec{\theta})$. 
    

    The art of back propagation is to reverse this process. We let $\mathcal{L}(\svec{\theta})$ provide information about the error of the output layer, that propagates backwards through the network in order to compute the gradient of the loss function for each layer, $\nabla^l_{\!\theta}\mathcal{L}(\svec{\theta})$. These gradients are used to update the weights and biases of each layer in such a way that when forward propagation is run again, the overall output loss will be lower. Over time, we propagate forwards a backwards in order to minimise the loss function, typically using stochastic gradient descent, as explained in \Sec{stochasticity}. The optimiser of choice (\Sec{tuning}) takes the gradients found from back propagation, (and hyperparameters) as inputs and optimises the weights and biases accordingly. A thorough walkthrough of the back propagation algorithm can be found in \citep[chapter 6.5]{Goodfellow2016}.


\subsection{Classification}\label{sec:classification}

\subsection{Logistic regression}\label{sec:logistic_regression}

The standard logistic function $p:\, \RR \to (0,1)$ may be written as 

\begin{equation}\label{eq:logistic_function}
    p(\xi) = \frac{1}{1+e^{-(\beta_0 + \beta_1\xi)}},
\end{equation}

and is indeed the sigmoid function in eq. \eqref{eq:sigmoid} if we substitute $\xi \to \beta_0 + \beta_1 \xi$. 
% beta or theta?
\fillertext

% \begin{equation}\label{eq:logistic_cost_function}
%     \mathcal{L}(\svec{\beta}) = - \sum_{i=1}^n \Big[ y^{(i)} \big(\beta_0+ \beta_1 x^{(i)}\big) - \log{\Big(1+e^{\beta_0 + \beta_1 x^{(i)}}\Big)} \Big]
% \end{equation}


\comment{This is rubbish}


\begin{equation}\label{eq:logistic_cost_function}
    \mathcal{L}(\svec{\theta}) = -  \frac{1}{n}\sum_{i=1}^n \Big[ y^{(i)} \big(\theta_0 + (X\svec{\theta})^{(i)}\big) - \log{\Big(1+e^{\theta_0 + \svec{\theta}\TT \vec{x}^{(i)}}\Big)} \Big]
\end{equation}

In terms of weights and biases ($ (\theta_0,  \svec{\theta} ) \to (\vec{b}, W)$),

\begin{equation}\label{eq:logistic_cost_function_NN}
    \mathcal{L}(\vec{b}, W) = -  \frac{1}{n}\sum_{i=1}^n \Big[ y^{(i)} \big(\vec{b} + W\TT \vec{h}^{l}\big) - \log{\Big(1+e^{\theta_0 + \svec{\theta}\TT \vec{x}^{(i)}}\Big)} \Big]
\end{equation}

\begin{equation}\label{eq:logistic_cost_function_NN2}
    \mathcal{L}(l) = -  \frac{1}{N_l} \Big[ (\vec{h}^{l})\TT\vec{a}^{l} - \log{\big(1+e^{\vec{a}^{l}}\big)} \Big]
\end{equation}

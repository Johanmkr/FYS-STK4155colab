\section{Theory}\label{sec:theory}

Linear regression assumes a linear relationship between a set of $p$ features $\vec{x}\in\RR[p]$ and an observed value $y\in\RR$. We assume there to exist a \textit{continous} function of the input $\vec{x}$ giving the output $\hat{y}$. The coefficients $\svec{\theta}\in\RR[p]$ that determine said function can be estimated using a variety of methods, as discussed in \projectOne. In any case, the aim is to minimise some loss function $\mathcal{L}(\svec{\theta};\,\hat{y}, y)$ with respect to this parameter vector ($\svec{\theta}$) describing what we sacrifice by using this exact model. 

What if the function we want to fit is \textit{discontinous}? We consider the binary situation where the observed $y$ only takes one of two discrete values; 0 or 1. Logistic regression proposes a model where the output $\hat{y}$ is obtained from  a probability distribution and subsequently a befitting total loss function that can be minimised with respect to a set of parameters $\svec{\theta}$. Now, instead of using the method-specific regression algorithms for finding the optimal $\svec{\theta}$, we can change modus operandi and focus solely on the minimisation of some objective function (e.g. a loss function such as the \checkthis{MSE}). For this purpose, we may use the very powerful procedure of steepest descent.

Where the actual (physical) relationship between some dependent and independent variable is not of \rephrase{utmost importance}, and the main aim is to predict the outcome given some setting, supervised learning problems may also be solved using neural networks. 

\par{*} NN - no parameter vector - allow multivariable y




\subsection{Stochastic gradient descent (SGD)}\label{sec:stochastic_gradient_descent}
    SGD and its subvariants are frequently used optimisation algorithms in machine learning \citep{Goodfellow2016}. The more basic algorithm known as gradient descent (GD) is technically a specific case of SGD\footnote{The case with number of minibatches $m=1$.} follows the gradient of some objective function $J$ downhill in some parameter space. The effect of introducing stochasticity is \rephrase{discussed} in section \ref{sec:stochasticity}. The result is a flexible way to locate the minima of \checkthis{any} $J$, exactly what we wished for. The ordinary least squares and Ridge schemes of linear regression that we discussed in \projectOne, are then implemented by using the mean squared error (MSE) function with an \lnorm[2] regularisation term,
    \begin{equation}\label{eq:linear_regression_cost_function}
        \mathcal{L}(\svec{\theta}) = \frac{1}{2n} \sum_{i=1}^n (\hat{y}^{(i)}-y^{(i)})^2 + \frac{\lambda}{2p} \sum_{j=1}^{p} \theta_j^2. 
    \end{equation}
    $\lambda$ is the penalty term set to zero for OLS and a small positive value for Ridge regression. The output $\hat{y}^{(i)}$ is resulting from some function evaluated at $\vec{x}^{(i)}$, which for our purposes reads $\hat{y}^{(i)} = [\vec{x}^{(i)}]\TT \svec{\theta}$.

    \fillertext

 
    \subsubsection{Plain gradient descent}\label{sec:plain_gradient_descent}
        The most basic concept is that of steepest descent. In order to find a minimum of a function $J=J(\svec{\xi})$, we follow the steepest descent of that function in $\svec{\xi}$-space, i.e. the direction of the negative gradient $-\nabla_{\!\xi} J(\svec{\xi})$. We thus have the iterative scheme to find minima,
        \begin{align}\label{eq:steepest_descent}
            \svec{\xi}_{k+1} = \svec{\xi}_k - \eta_k\nabla_{\!\xi} J(\svec{\xi}_k),
        \end{align}
        where the learning rate $\eta_k$ may follow a schedule in $k$ or stay constant. In the following, we consider a constant global\footnote{Emphasising "global" here to distinguish from the \textit{actual} rate of learning (or step size) which may depend on the specific update rule we choose.} learning rate $\eta_k=\eta$.

        What we would like to minimise is the cost function $\mathcal{L}(\svec{\theta})$ which is a function of the parameters $\svec{\theta}$ which we are trying to estimate. If we define $\mathcal{A}_k \equiv \nabla_{\!\theta} \mathcal{L}(\svec{\theta}_k)$ to be the direction and magnitude of the steepest ascent in parameter space, eq. \eqref{eq:steepest_descent} reads 
        \begin{equation}\label{eq:update_rule_general}
            \begin{split}
                \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k\,;  \\
                &\quad \vec{v}_k = -\eta\mathcal{A}_k,
            \end{split}
        \end{equation}
        for substitutions $J\to\mathcal{L}$ and $\svec{\xi}\to \svec{\theta}$. For a sufficiently small $\eta$, this method will converge to a minimum of $\svec{\theta}$. However, since we may not know the nature of $\mathcal{L}$ in parameter space, there is a risk that said extremum is just a local and not a global minimum. The steepest descent method in eq. \eqref{eq:update_rule_general} is a deterministic method, which means we may get stuck in a local minimum. There are several ways around this, and one such way is to include an element of randomness in the computations, as we will see in section \ref{sec:stochasticity}.

    \subsubsection{Momentum}\label{sec:momentum}
        From eq. \eqref{eq:update_rule_general} we have that the movement in parameter space is given by $\vec{v}_k$ (the negative of which), which describes the direction and magnitude of the steepest ascent in parameter space. Sometimes we might want to move larger distances in one step. This can be achieved by introduction momentum: We add an addition term to $\vec{v}_k$ which lets us rewrite eq. \eqref{eq:update_rule_general} as
        \begin{equation}\label{eq:momentum_GD_algorithm}
            \begin{split}
                \svec{\theta}_{k+1} &= \svec{\theta}_k + \vec{v}_k\, ; \\
                &\quad \vec{v}_k =\gamma\vec{v}_{k-1} - \eta\mathcal{A}_k,
            \end{split}
        \end{equation}
        where $\gamma$ is a momentum parameter and $\eta$ is the same global learning parameter as before. The basic idea is that this with this method we "overshoot" the descending step length in the direction of the previous step, with a magnitude that is controlled by $\gamma$. By doing this, we may reach the desired minimum with fewer iterations. 

        % There are several modifications we can do to optimise this algorithm. In the Nesterov momentum algorithm (NAG) one applies the following adjustment to the gradient in eq. \eqref{eq:momentum_GD_algorithm}:
        % \begin{equation}\label{eq:nag}
        %     \begin{split}
        %         \mathcal{A}_k &\to\nabla_{\!\tilde{\theta}}\mathcal{L}(\tilde{\svec{\theta}}_k) \, ;\\ 
        %         & \quad \tilde{\svec{\theta}}_k = \svec{\theta}_k + \gamma\vec{v}_{k-1} \,Â ;
        %     \end{split}
        % \end{equation}

        % This is analagous to the adjustment needed to go from forward Euler to Euler-Cromer as numerical intregration method.

        % \fillertext

        

    \subsubsection{Stochasticity}\label{sec:stochasticity}
        There are several weaknesses to the plain gradient descent, perhaps the largest is the computational expense of on large datasets and its sensitivity of initial conditions and learning rates. If $\mathcal{L}(\svec{\theta})$ has numerous local minima, we will find one minimum only per set of initial conditions, and we have no good way of saying whether this minimum is global or not. One way of overcoming this is by adding stochasticity to the gradient descent algorithm. 

        The main idea is that with the $n$ datapoints which we have in a dataset $\mathcal{D}$, we can create $m$ subsets, meaning that we have $n/m$\footnote{Give or take; needs to be an integer.} datapoints in each \textit{minibatch}, denoted $\mathcal{B}_j$ for $j\in\{1,2,\dots,m\}$, s.t. $\bigcup_{j=1}^m \mathcal{B}_j = \bigcap_{j=1}^m \mathcal{B}_j = \mathcal{D}$. We recognise that we may write the total cost function as a sum over all data points $\vec{x}^{(i)}$ for $i\in[1,n]$, 
        \begin{equation}
            \mathcal{L}(\svec{\theta}) =\sum_{(\vec{x}, y)\in \mathcal{D}} l\big(f(\vec{x};\,\svec{\theta}), y \big) = \sum_{i=1}^n l_i(\svec{\theta}),
        \end{equation}
        where $l_i(\svec{\theta}) = l\big(f(\vec{x}^{(i)};\svec{\theta}); \, y^{(i)}\big)$ is the per-example loss function. Thus, its gradient is written
        \begin{equation}
            \mathcal{A} = \nabla_{\!\theta} \mathcal{L}(\svec{\theta}) = \sum_{i=1}^n \nabla_{\!\theta}l_i(\svec{\theta}).
        \end{equation}
        Now we may approximate the gradient of the cost function by only summing over the data points in a minibatch picked at random:
        \begin{equation}\label{eq:sgd_gradient}
            \begin{split}
            \mathcal{A}_k &= \sum_{j=1}^{m}\mathcal{A}_k^j\, ; \\
            \mathcal{A}_k &\to \mathcal{A}^j_k = \sum_{i:\vec{x}^{(i)} \in\mathcal{B}_j }\nabla_{\!\theta} l_i(\svec{\theta}_k)\, ;
            \end{split}
        \end{equation}
        The estimate $\mathcal{A}_k^j \approx \mathcal{A}_k$ can be used in our algorithm to ensure stochasticity and relieve computational pressure.
    
    \subsubsection{Optimising the learning rate}\label{sec:tuning}
    
    There is in general no way of knowing a priori what value $\eta$ or $\gamma$ should take. Tuning such hyperparameters makes up a significant part of the work in a supervised learning problems. We can mitigate parts of the struggle with hyperparameter adjustment by using an algorithm with adaptive learning rates. We present a few such schemes in short below, that is different ways of calculating $\vec{v}$ in eq. \eqref{eq:update_rule_general}. All require an original learning rate $\eta$ and a small number $\epsilon$ for numerical stability. 

    \begin{enumerate}[leftmargin=0pt,labelwidth=!,labelsep=.05em]
        \item[]\textbf{AdaGrad} \citep[algorithm 8.4]{Goodfellow2016}, from "adaptive gradient (algorithm)", adapts $\eta$ individually to the components of $\svec{\theta}$, scaling them as $\eta \to \eta' \sim \nicefrac{\eta}{\nabla_{\! \theta} \mathcal{L}}$ (mind the simplification). This method is famously a \rephrase{trouper} in convex settings, but has the unfortunate habit of prematurely deciding on the best model in a nonconvex setting. By default: $\epsilon=10^{-7}$.
        \item[]\textbf{RMSProp} \citep[algorithm 8.5]{Goodfellow2016}, from "root mean squared propagation", is a modification to the AdaGrad algorithm. Where AdaGrad performs good, this one learns slow in comparison. However, RMSProp outperforms AdaGrad in nonconvex situations. This improvement introduces an additional hyperparameter $\rho$, a decay rate controlling the length scale of the moving average. By default: $\rho = 0.9$ and $\epsilon=10^{-7}$.
        \item[]\textbf{Adam} \citep[algorithm 8.7]{Goodfellow2016}, from "adaptive moments", calculates the update based on both the first-order momentum, the same as in momentum, and the second-order momentum, much like in RMSProp. We need two hyperparameters $\rho_1$ and $\rho_2$ for this optimisation scheme, representing the decay rate of the first and second moment, respectively. By default: $\rho_1 = 0.9$, $\rho_2=0.999$ and $\epsilon=10^{-8}$.
    \end{enumerate}
    
    The choice of default values is inspired by \citep[chapter 8.5]{Goodfellow2016}. 

    
    
\subsection{Neural Network (NN)}\label{sec:neural_network}

We have so far discussed regression and given a lot of attention to the coefficients $\svec{\theta}$ that we assume describe some physical relationship between a set of feature values and a target. More complex problems require more complex models, and many phenomena may not even be possible to describe with a smooth function. Deep learning models usually pay more attention to the output, in this section denoted $\hat{\vec{y}}$ to allow multivariable outputs\footnote{For completeness. Will not be relevant to think of the output as a vector in our analysis.}, after training, that is. 
%To connect the following with what we have looked at so far, we can think of the weights and  the parameter vector $\svec{\theta}$

    \subsubsection{Basics}\label{sec:basics}

    A feedforward NN (FFNN) is typically built by composing together several functions into a chain of function. Associated with this model is a directed acyclic graph (DAG) describing the explicit structure. The depth of the model is determined by the length of the abovementioned chain. Each function represents a layer in the network. The final layer of an FFNN is the output layer, and the layers between the input (prior to the first) and the output layer are called hidden layers. \citep{Goodfellow2016}

    The structure of such a chain-based architecture is described by the $L-1$ hidden layers $\vec{h}^l \in \RR[N_l],\,l=1,2, \dots, L-1$, given by
    \begin{subequations}\label{eq:nn_layers}
        \begin{align} 
            \vec{h}^0 &= \vec{x}^{(i)}\,; \\% dunno
            \vec{h}^1 &= g_1 \big((W^{0\to 1})\TT \vec{h}^0 + \vec{b}^1\big)\,; \\
            \vec{h}^2 &= g_2 \big( (W^{1\to 2})\TT \vec{h}^1 + \vec{b}^2\big)\,; \\
            &\vdots \nonumber \\
            \vec{h}^{L} &= g_L \big((W^{{L\!-\!1\to L}})\TT \vec{h}^{L\!-\! 1} + \vec{b}^L\big)\,;             
        \end{align}
    \end{subequations}
    where we defined $\vec{h}^0$ and $\vec{h}^L$ to be the input and output layer, respectively. 

    The matrix of weights $W^{l-\! 1\to l} \in \RR[N_{l-\!1} \cross N_l]$ applies weights and dimension corrections to the previous layer $\vec{h}^{l-\! 1}\in \RR[N_{l-\! 1}]$ so that the activation function $g_l \,:\, \RR[N_l] \to \RR[N_l]$ can accept the input. The bias $\vec{b}^l \in \RR[N_l]$ may be interpreted as a safety mechanism of the neurons to prevent their layer value to become zero, and is typically set to a small non-zero value \citep{Goodfellow2016}. 

   



    \subsubsection{Activation functions}\label{sec:activation_function}


    A layer $\vec{h}^l$ has an associated activation $\vec{a}^l \in \RR[N_l]$ which is a function of the previous layer values, $\vec{h}^{l-\!1}$, the weights, $W^{l-\!1\to l}$, and the biases linked with each neuron, $\vec{b}^l$. The activation is passed as argument to the activation function $g_l$ whose job is to perform the affine transformation from one layer to another in a NN. In eq. \eqref{eq:nn_layers} the activation is $\vec{a}^l = (W^{l-\! 1\to l})\TT\vec{h}^{l -\!1} + \vec{b}^l$, which is valid for $l= 1, 2,\dots L$. Note that the weight matrix $W^{l-\! 1\to l}$ is associated with both the current and previous layer. We can rewrite the formula in eq. \eqref{eq:nn_layers} as the more compact expression:
    \begin{equation}
        \begin{split}
            \vec{h}^0 &= \vec{x} \,,\quad \vec{h}^l =  g_l(\vec{a}^l)\,, \, \, l=1,2, \dots L \,; \\
            &\quad \vec{a}^l = W^{l\leftarrow l-\!1}\vec{h}^{l -\!1} + \vec{b}^l \,;
        \end{split}
    \end{equation}
    where $W^{l\leftarrow l-\!1} \equiv (W^{l-\! 1\to l})\TT$. The output is $\hat{\vec{y}}^{(i)} = \vec{h}^L \in \RR[N_L]$.

    We present some examples of commonly used activation functions:

    \begin{subequations}\label{eq:activation_functions}
        \begin{align}
            &\sigma(\svec{\xi}) = \frac{1}{1+e^{-\svec{\xi}}} = 1- \sigma(-\svec{\xi})\label{eq:sigmoid}\\
            &\tanh(\svec{\xi}) = \frac{e^{2\svec{\xi}}-1}{e^{2\svec{\xi}}+1} = 2\sigma(2\svec{\xi}) -1\label{eq:tanh} \\
            &\mathrm{ReLU}(\svec{\xi}) = \max(0,\svec{\xi}) = \begin{cases}
                \svec{\xi},\quad &\svec{\xi} >0 \\
                0,\quad &\svec{\xi} \leq 0
            \end{cases} \label{eq:relu}\\
            &\mathrm{ReLU}^*(\svec{\xi})  = \begin{cases}
                \svec{\xi},\quad &\svec{\xi} >0 \\
                0.01\svec{\xi},\quad &\svec{\xi} \leq 0
            \end{cases} \label{eq:leaky_relu}
        \end{align}
    \end{subequations}

    % add linear??

    Assuming some $\svec{\xi} \in \RR[K]$, the set of expressions \eqref{eq:activation_functions} show some well-known activation functions $\RR[K] \to \RR[K]$. To translate into NN components, set $K\to N$, $\svec{\xi} \to \vec{a}$ and e.g. $g = \tanh$. 
    The oldest and probably most famous is the slow-learning sigmoid function $\sigma$ in eq. \eqref{eq:sigmoid}. The hyperbolic tangent in eq. \eqref{eq:tanh} is closely related to the sigmoid, and is typically performing better \citep{Goodfellow2016}. The ReLU (eq. \eqref{eq:relu}) or leaky ReLU (eq. \eqref{eq:leaky_relu}) activation function provides output of the type that is easy to interpret as it resembles the linear unit. ReLU typically learns fast, but has the the unfortunate habit of killing neurons. That is to say, some neurons are deactivated for any input. The leaky ReLU can omit this issue somewhat, but the hatch is a perfomance reduction.

    





    \subsubsection{Back propagation}\label{sec:back_propagation}


    The information in an FFNN accepting input $\vec{x}$ to produce output $\vec{\hat{y}}$ is flowing \textit{forward} \citep{Goodfellow2016}, hence the name. The initial information from $\vec{x}$ propagates through the hidden layers resulting in the production of $\vec{\hat{y}}$ which is the output of the final layer. This information flow is called forward propagation or forward pass. Training the network (tuning the weights and biases) consist of running forward propagation and compare the resultant output $\vec{\hat{y}}$ with the desired output $\vec{y}$, i.e. evaluate the loss function, $\mathcal{L}(\svec{\theta})$. 
    

    The art of back propagation is to reverse this process. We let $\mathcal{L}(\svec{\theta})$ provide information about the error of the output layer, that propagates backwards through the network in order to compute the gradient of the loss function for each layer, $\nabla^l_{\!\theta}\mathcal{L}(\svec{\theta})$. These gradients are used to update the weights and biases of each layer in such a way that when forward propagation is run again, the overall output loss will be lower. Over time, we propagate forwards a backwards in order to minimise the loss function, typically using stochastic gradient descent, as explained in \Sec{stochasticity}. The optimiser of choice (\Sec{tuning}) takes the gradients found from back propagation, (and hyperparameters) as inputs and optimises the weights and biases accordingly. A thorough walkthrough of the back propagation algorithm can be found in \citep[chapter 6.5]{Goodfellow2016}.

\subsection{Linear Regression}\label{sec:regression}
    When using a FFNN for a regression problems we opt to fit a function to a certain data set. One approach to this is to let the number of features be the dimensionality of the input data, and the number of data points be the data points in the domain we are considering. In this investigation we will use the two dimensional Franke function from \projectOne\footnote{Equation (10) in the paper.} on a $20\cross 20$ grid (more on this later).
    
    Since we want to fit a function to data points, the obvious way of measured the error from the output layer is by considering the mean squared error (MSE). Thus, our loss function will as given in \Eq{linear_regression_cost_function} with a tunable hyperparameter $\lambda$. 

    We may use a varying number of hidden layers and neurons, depending on the data we are trying to fit. The architecture of the network is a problem dependant feature and must be tested for. The same goes for the activation functions described in \Eq{activation_functions}. However, the output function $g_L$ can just to be the linear function: $g_L(\vec{a}^L) = \vec{a}^{L}$. 
    
    
    \feltcute{This means that the number of features is $p=2$ input dimensions, for $n=400$ data point. Each data point (a combination of $p$ features) will yield one single output. }

\subsection{Classification}\label{sec:classification}
    For the case of classification, we typically have many features in the input data, which results in a more complex design matrix. The output of the network is structured into two main classes, binary and multivariate classification. In binary classification we have one single output node that ideally should be either 0 or 1. We achieve this by having a sigmoid output function. In multivariate classification on the other hand, we have multiple output nodes and a probability distribution between them. This probability distribution is found using the Softmax function. However, the data set we will analyse is the Wisconsin Breast Cancer dataset \citep{scikit-learn} which needs binary classification. Thus the main focus will be on the binary classification technique. 

    When evaluating the loss we use \Eq{logistic_regression_cost_function}  which is known as the cross entropy or log loss function. 

\subsection{Logistic regression}\label{sec:logistic_regression}

The standard logistic function $p:\, \RR \to (0,1)$ may be written as 

\begin{equation}\label{eq:logistic_function}
    p(\xi) = \frac{1}{1+e^{-(\beta_0 + \beta_1\xi)}},
\end{equation}

and is indeed the sigmoid function in eq. \eqref{eq:sigmoid} if we substitute $\svec{\xi} \to \beta_0 + \beta_1 \xi$ ($\beta_0, \beta_1 \in \RR$ arbitrary constants). This can be generalised to work with multivariable inputs $\vec{x}$: We include an intercept $\theta_0$ and let $\svec{\theta} = (\theta_1, \, \theta_2, \, \dots,\, \theta_p)$ be the same as before. Then,

\begin{equation}\label{eq:logistic_function_fit}
    p(\vec{x};\,\theta_0, \vec{\theta}) = \frac{1}{1+e^{-(\theta_0 + \vec{x}\TT \svec{\theta})}},
\end{equation}

so that $\hat{y}^{(i)} = p(\vec{x}^{(i)})$.

\begin{equation}\label{eq:logistic_regression_cost_function}
    \begin{split}
        \mathcal{L}(\svec{\theta}) =& -  \frac{1}{2n}\sum_{i=1}^n \Big[ y^{(i)} \log{\hat{y}^{(i)}} + \big(1-y^{(i)}\big) \log{\big(1-\hat{y}^{(i)}\big)}\Big] \\
        &+ \frac{1}{2p}\sum_{j=0}^p \theta_j^2
    \end{split}
\end{equation}

for this

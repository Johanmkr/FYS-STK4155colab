\section{Conclusion}\label{sec:conclusion}

For linear regression on the Franke function, the optimal model is an NN with $L-1=1$ hidden layer with the number of neurons $N_1=30$. The hyperparameters are tuned to be $\eta=10^{-1}$, $\lambda=10^{-4}$. We use the sigmoid activation function and train the model over 700 epochs using SGD with RMSProp optimiser with $\rho=0.9$ using $m=2$ minibatches. The resulting performance measure $\mathrm{MSE} = 0.052$. This is significantly better than 0.15, which was obtained with OLS on the same function, with comparable noise in \projectOne. This is only one of many models that could deliver an equally good result. It is fairly computational efficient, and accurately fits a function to the data. 

For the classification task, we found the optimal model to be an NN with $L-1=2$ hidden layers for which $N_1=N_2=10$. The hyperparameters are tuned to be $\eta=10^{-3}$ and $\lambda=10^{-6}$. We use the ReLU activation function and train the model over 900 epochs using SGD with RMSProp optimiser ($\rho=0.9$) having $m=5$ minibatches. The result is a model with $\mathrm{Accuracy}=1$, which is the best possible accuracy. 

For logistic regression we use an NN with no hidden layers ($L-1=0$), and sigmoid activation function ($g_L=\sigma$). This as well manages to perform with $\mathrm{Accuracy}=1$, having $\eta=10^{-3}$ and $\lambda=10^{-8}$. Since this is a simpler model than the NN of depth $L=3$, we may conclude that logistic regression does an equally good job in this case. 

A continuation of this work would be to troubleshoot the code for implementation errors and perform more simulation of different network type, initialise the weights and biases differently and tune the architecture and hyperparameters in different ways and orders. Another natural extension would be to make the neural network able to perform multivariate classification. 
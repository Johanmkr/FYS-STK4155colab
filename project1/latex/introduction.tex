\section{Introduction}\label{sec:intro}
Linear regression is the gateway to statistical analysis, and in this investigation we will perform three types of linear regression: ordinary least squares (OLS), Ridge, and Lasso regression, on two different data sets: The Franke function and terrain data of parts of the Grand Canyon in the United States. Common for all is, in addition to minimising error, that we will fit a two-dimensional polynomial onto the two data sets by finding an optimal set of parameters $\optbeta$, which in our case will be the coefficients of the different terms in the polynomial. How we determine these parameters depend on the regression method of choice. The way of measuring error (deviation from data), the cost function, varies between the different regression methods. Ridge and Lasso regression are so-called regularisation methods, allowing us to deal with more complex models, reducing the chance of over-fitting. Determining the best model is very dependant on the data set at hand. In \Sec{SVD} we state the singular value decomposition from linear algebra. This comes in handy when explaining the regression models in \Sec{regression}, especially the penalty term involved in the regularisation of the Ridge and Lasso schemes. We also explain resampling methods in \Sec{resampling}, which are useful when assessing the accuracy of our model. The main analysis is given in \Sec{analysis} with an introduction to the data and noise of the Franke function in \Sec{data}, description of how and why we split the data in \Sec{splitting}, how we set up the model and design matrix in \Sec{model}, and how and why we scale the data in \Sec{scaling}. We then carry out the analysis of the Franke function in \Sec{reganalysis_franke}, and of the terrain data in \Sec{reganalysis_real_data}. We draw conclusions in \Sec{conclusion}. At last, we link to the code and list all figures, as well as captions.
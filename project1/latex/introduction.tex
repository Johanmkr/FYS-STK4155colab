\section{Introduction}\label{sec:intro}
Linear regression is the gateway to statistical analysis, and in this investigation we will perform three types of linear regression: Ordinary Least Squares (OLS), Ridge, and Lasso regression,on two different data sets: The Franke function and terrain data of parts of the Grand Canyon in the United States. Common for all is that we will try to fit a two-dimensional polynomial onto the two data sets by finding an optimal set of parameters $\optbeta$, which in our case will be the coefficients of the different terms in the polynomial. How we determine these parameters depend on the regression method of choice. Common for all is to minimise the error, deviation from the true data. The way of measuring this error, the cost function, varies between the different regression methods. Ridge and lasso regression are so-called regularisation methods, which allows us to deal with more complex models, reducing the chance of over-fitting. Determining the best model is very dependant on the data set at hand. In \Sec{SVD} we state the singular value decomposition from linear algebra. This comes in handy when explaining the regression models in \Sec{regression}, especially the penalty term involved in the regularisation of the ridge and lasso methods. We also explain resampling methods in \Sec{resampling}, which are useful when assessing the accuracy of our model. The main analysis is given in \Sec{analysis} with an introduction to the data and noise of the Franke function in \Sec{data}, description of how and why we split the data in \Sec{splitting}, how we set up the model and design matrix in \Sec{model}, and how and why we scale the data in \Sec{scaling}. We then carry out the analysis of the Franke function in \Sec{reganalysis_franke}, and of the terrain data in \Sec{reganalysis_real_data}. We make a conclusion for each data set in \Sec{conclusion}. At the end, a list of figures with captions is provided. 
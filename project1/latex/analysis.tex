\section{Analysis}\label{sec:analysis}

    \subsection{Data and noise}\label{sec:data}
        The function, onto which we are trying to fit a model, is the Franke Function(cite this). It is defined as follows:
        \begin{align}\label{eq:francefunc}
            f(x,y) &= \frac{3}{4}\exp{-\frac{(9x-2)^2}{4}-\frac{(9y-2)^2}{4}} \nonumber\\
            &+ \frac{3}{4}\exp{-\frac{(9x+1)^2}{49}-\frac{(9y+1)}{10}} \nonumber\\
            &+ \frac{1}{2}\exp{-\frac{(9x-7)^2}{4}-\frac{(9y-3)^2}{4}} \nonumber\\
            &-\frac{1}{5}\exp{-(9x-4)^2-(9y-7)^2}.
        \end{align}
        In order to generate a dataset we will use $N$ uniformly distributed values of $x,y\in[0,1]$. We will also add some normally distributed noise $\epsilon = \eta\mathcal{N}(\mu,\sigma^2) = \eta\mathcal{N}(0,1)$ to $f(x,y)$, where $\eta$ is a strength parameter controlling the amplitude of the added noise. The full description of our data then become:
        \begin{align}\label{eq:datadescription}
            \vec{y} &= f(x,y) + \eta\mathcal{N}(0,1) \nonumber \\
            &= f(\vec{x}) + \epsilon
        \end{align}
        where $\vec{x} = (x,y)$. 
        From section \Sec{regression} we have a model for this data: $\tilde{\vec{y}} = X\hat{\svec{\beta}}$ where $X$ is the design matrix and $\hat{\svec{\beta}}$ are the optimal parameters which we are trying to determine. 

        We visualise the data both with and without noise. Figure \Fig{franke_function} shows the Franke function without any noise ($\eta=0$), plotted for uniformly distributed $x$ and $y$, where $N=20$. We could use more data points, but for the sake computational efficiency we use a $20\cross20$ grid throughout this analysis.  Figure \Fig{franke_function_noise} show the same function, for the same points but now with an added noise of $\eta=0.1$.

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/data3D_no_noise.pdf}
            \caption{The Franke function plotted on a grid where $N=20$ and $\eta=0$. Since we scale the data, the $z$ becomes arbitrary and we choose to leave it out. The important information is the shape of the graph which is smooth when there is no noise. }
            \label{fig:franke_function}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/data3D.pdf}
            \caption{The Franke function with added noise plotted on a grid where $N=20$ and $\eta=0.1$. Since we scale the data, the $z$ becomes arbitrary and we choose to leave it out. The important information is the shape of the graph which becomes quite ragged even for this small level of noise}
            \label{fig:franke_function_noise}
        \end{figure}

    \subsection{Data splitting}\label{sec:splitting}
        In order to test our estimate of $\hat{\svec{\beta}}$ we reserve some of the data for testing. We thus divide our data set into a part for testing a part for training. We select 80 \% of the data for training and the remaining 20 \% for testing the data. The data is split at random. 




    \subsection{Model and design matrix}\label{sec:model}
        The design matrix $X$ has dimensionality ($n\cross p$) where $n$ represents the data points and $p$ the features of the model. We have already split the data set into training and testing, and must therefore create two design matrices: $X^{\text{train}}$ and $X^{\text{test}}$. 
        \\
        We want our model to be a two-dimensional polynomial of order $d$:
        \begin{align*}
            P_d(x,y) = \beta_0 + \sum_{l=1}^d\sum_{k=0}^{l} \beta_jx^{l-k}y^k
        \end{align*}
        where $j\in[1,p]$ and $p=(d+1)\cdot\left[(d+2)/2\right]$ is the number of features, or number of terms in a two dimensional polynomial. $\beta_0$ is our intercept (constant term). From this we set up the design matrix with the following terms (THIS IS WRONG; CORRECT THIS):
        \begin{align}\label{eq:designmatrixequation}
            X_{ij} = \sum_{l=1}^d\sum_{k=0}^{l} x^{l-k}y^k
        \end{align}
        The design matrix is set up without an intercept (constant term) (WHY?). Since we need to set up to design matrices, they account for a different amount of data points, and thus $n$ will be different between the two, but $p$ is the same. 





    
    \subsection{Scaling}\label{sec:scaling}
        We want to scale both our data and the design matrices because the data obtained from either the Franke function, or from some other source often vary a great deal in magnitude. Since the design matrices are set up in order to fit data to a polynomial of degree $d$ in two dimensions, we want to be sure that no term is given more emphasis than the others. In addition, when working with multi-dimensional data and design matrices we want to standardize the features as much as possible to ensure equal (relative) emphasize and treatment of the dimensions. We use the scaling technique \textit{standardization} or \textit{Z-score normalization} which makes the mean of each feature equal to zero, and the their variances to be of unit length. For our observed data $\vec{y}$ this mean:
        \begin{align*}
            \vec{y}' = \frac{\vec{y}-\mu_{\vec{y}}}{\sigma_{\vec{y}}}
        \end{align*}
        where $\vec{y}'$ is now our scaled data. For the design matrices, this must be done for each feature, i.e. for each column.  Mathematically, if $X_j$ is column $j$ of the original design matrix $X$, $j\in[1,p]$:
        \begin{align*}
            X_j' = \frac{X_j-\mu_{X_j}}{\sigma_{X_j}}.
        \end{align*}
        We do this for all the columns and end up with the scaled design matrix $X'$. Since $\optbeta$ is a function of the design matrix of the training data: $X^{\text{train}}$ and the data $\vec{y}$, we need to scale both the training and test data with respect to the mean and standard deviation of each column of the train data:
        \begin{align*}
            X_j'^{\text{ train}} &= \frac{X_j^{\text{train}}-\mu_{X_J^\text{train}}}{\sigma_{X_j^{\text{train}}}} \\
            X_j'^{\text{ test}} &= \frac{X_j^{\text{test}}-\mu_{X_J^\text{train}}}{\sigma_{X_j^{\text{train}}}}
        \end{align*}
        This scaling will ensure that we treat the data in the same way, no matter the actual numerical value of the data. 

    % \begin{align*}
    %     \optbeta' = \optbeta\frac{\sigma_X}{\sigma_{\vec{y}}} \implies \optbeta = \optbeta'\frac{\sigma_{\vec{y}}}{\sigma_X}
    % \end{align*}


    \subsection{Regression analysis for Franke function}\label{sec:reganalysis_franke}

        \subsubsection{OLS}\label{sec:olsanalysis}
            We start by performing an Ordinary Least Square regression analysis, as outlined in section \Sec{OLS}, where we find $\hat{\svec{\beta}}^\text{OLS}$ from \Eq{optimal_beta_ols} and $\variance{\optbeta^{\text{OLS}}}$ from \Eq{variance_of_optimal_beta_ols}. The error can be estimated with the standard deviation $\vec{\sigma}_{\beta} = \variance{\optbeta^{\text{OLS}}}^{1/2}$. 

            We have scaled the data and removed the intercept, thus we do not concern ourselves with $\beta_0$, so $\svec{\beta}^{\text{OLS}} = (\beta_1, \beta_2, \beta_3, \dots, \beta_{p-1})$. We train models, i.e. we find $\optbeta^{\text{OLS}}$ for polynomials up to order $5$. The values of these optimal parameters are plotted in \Fig{beta_with_standard_deviation}, with error bars of one standard deviation. We notice that the variation of the feature parameters increase as the polynomial degree of the model increase. This is expected because when the complexity of the model increase, it want to traverse more points exactly. We further see that the coefficients of the same features more or less have the same sign for the different polynomial degrees. This is a sign of a stable model, as we do not want these coefficient to change much when we use different polynomial degrees for our model. WHY??

            \begin{figure}
                \includegraphics[width=\linewidth]{franke/beta_ols.pdf}
                \caption{Numerical value of the feature parameters $\beta$, with 1$\sigma$ error bars, for polynomials up to order $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$). We note that the parameters for similar features tend to have the same sign across models, however there is an increase in parameter magnitude. This is clearly visible for $d=5$. }
                \label{fig:beta_with_standard_deviation}
            \end{figure}

            Having found the optimal parameter $\optbeta^{\text{OLS}}$ we can make predictions by computing $\tilde{\vec{y}} = X\optbeta^{\text{OLS}}$ on both the training data and the test data. In order to say something about the quality of these predictions we compute the mean square error and the $R^2$ score from equations \Eq{MSE} and \Eq{R2} respectively. The result of this is shown in \Fig{mse_and_r2_for_order5}. We see that the MSE decreases and the $R^2$ score increases (towards 1) as the polynomial degree increase. This signifies that the data we are trying to model (the Franke function) shows a complex behaviour, and is not very well modelled by a low-degree polynomial. From this plot a lone it is fair to deduce that the higher the polynomial degree the better the model. However, we will see that this is not necessarily the case. 

            \begin{figure}
                \includegraphics[width=\linewidth]{franke/MSE_R2_scores_ols.pdf}
                \caption{MSE and $R^2$ scores for train and test data for polynomials up to order $d=5$ for the OLS analysis of the Franke function ($N=20$). We  see that for the polynomial degrees present, the MSE decrease, while the $R^2$ increase towards 1 for increasing polynomial  degree. }
                \label{fig:mse_and_r2_for_order5}
            \end{figure}
            
            
            SOMETHING ABOUT NOISE AND STUFF WHEN THE plot in \Fig{mse_for_different_noise_ols}

            \begin{figure}
                \includegraphics[width=\linewidth]{franke/error_vs_noise_ols.pdf}
                \caption{MSE for train and test data for polynomials up to order $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$), for different noise parameters $\eta\in [0.0001, 0.001, 0.01, 0.1,  1]$. The MSE is large for noisy function (large $\eta$), but decreasing with polynomial degree.}
                \label{fig:mse_for_different_noise_ols}
            \end{figure}


            From \Fig{mse_and_r2_for_order5} we got the impression the higher the polynomial degree, the better the model. We want to investigate this further and look at how the (MEAN) MSE behaves for polynomials of order up to 20. The model is trained on the training data, but the MSE is evaluated for both the training and test data. The result is shown in \Fig{model_complexity_ols}. The MSE of the training data decrease as the polynomial degree increase. This is not a surprise, since the model will traverse more points of the training data exactly as the complexity ($d$) increase. The MSE of the testing data however, seem to decrease at first, but then rapidly increase as the polynomial degree increase. This is similarly explained with the model being too tailored to the training data. When applied to the test data (which consist of completely different pieces of data), the model fails to make accurate predictions. This is an indicator of a phenomenon called \textit{overfitting}: when the model is so tailored to the training data it is unable to make accurate predictions on other, similar data. For a more reliable result, the data in figure \Fig{model_complexity_ols} is generated by bootstrapping the training data with $k=400$ as explained in \Sec{bootstrap}

            \begin{figure}
                \includegraphics[width=\linewidth]{franke/MSE_ols_BS.pdf}
                \caption{MSE for train and test data for polynomial up to order $d=12$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$). When we increase the model complexity  up to order $d=12$ we easily  spot that the MSE for train data starts to diverge, which could be a sign of overfitting. }
                \label{fig:model_complexity_ols}
            \end{figure}


            \begin{figure}
                \includegraphics[width=\linewidth]{franke/tradeoff_ols.pdf}
                \caption{Bias Variance}
                \label{fig:bias_variance_ols}
            \end{figure}


            \begin{figure}
                \includegraphics[width=\linewidth]{franke/MSE_ols_CV.pdf}
                \caption{Cross-validation}
                \label{fig:cross-validation_ols}
            \end{figure}


            \begin{figure}
                \includegraphics[width=\linewidth]{franke/MSE_hist_OLS.pdf}
                \caption{MSE Hist ols}
                \label{fig:mse_hist_ols}
            \end{figure}


            \begin{figure}
                \includegraphics[width=\linewidth]{franke/beta_hist_OLS.pdf}
                \caption{Beta Hist ols}
                \label{fig:beta_hist_ols}
            \end{figure}


        

        \subsubsection{Ridge}\label{sec:rigdeanalysis}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/MSE_ridge_CV.pdf}
            \caption{Cross-validation for Ridge regression.}
            \label{fig:cross-validation_ridge}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/MSE_ridge_BS.pdf}
            \caption{Model complexity for Ridge regression. (RIKTIG PLOT?)}
            \label{fig:model_complexity_ridge}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/tradeoff_ridge.pdf}
            \caption{Bias-variance for Ridge regression.}
            \label{fig:bias_variance_ridge}
        \end{figure}

        \subsubsection{Lasso}\label{sec:lassoanalysis}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/MSE_lasso_CV.pdf}
            \caption{Cross-validation for Lasso regression.}
            \label{fig:cross-validation_lasso}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/MSE_lasso_BS.pdf}
            \caption{Model complexity for Lasso regression. (RIKTIG PLOT?)}
            \label{fig:model_complexity_lasso}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{franke/tradeoff_lasso.pdf}
            \caption{Bias-variance for Lasso regression.}
            \label{fig:bias_variance_lasso}
        \end{figure}

    % \subsection{Applying best fit model}\label{sec:applybestmodel}


    \subsection{Regression analysis for real terrain data}\label{sec:reganalysis_real_data}

    \begin{figure}
        \includegraphics[width=\linewidth]{terrain/data3D.pdf}
        \caption{Scaled terrain data representing a part of the Grand Canyon, downloaded from \citep{EarthExplorer}.}
        \label{fig:gc_data}
    \end{figure}
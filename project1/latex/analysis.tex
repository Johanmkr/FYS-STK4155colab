\section{Analysis}\label{sec:analysis}

    \subsection{Data and noise}\label{sec:data}
        The function, onto which we are trying to fit a model, is the Franke Function(cite this). It is defined as follows:
        \begin{align}\label{eq:francefunc}
            f(x,y) &= \frac{3}{4}\exp{-\frac{(9x-2)^2}{4}-\frac{(9y-2)^2}{4}} \nonumber\\
            &+ \frac{3}{4}\exp{-\frac{(9x+1)^2}{49}-\frac{(9y+1)}{10}} \nonumber\\
            &+ \frac{1}{2}\exp{-\frac{(9x-7)^2}{4}-\frac{(9y-3)^2}{4}} \nonumber\\
            &-\frac{1}{5}\exp{-(9x-4)^2-(9y-7)^2}.
        \end{align}
        In order to generate a dataset we will use $N$ uniformly distributed values of $x,y\in[0,1]$. We will also add some normally distributed noise $\epsilon = \eta\mathcal{N}(\mu,\sigma^2) = \eta\mathcal{N}(0,1)$ to $f(x,y)$, where $\eta$ is a strength parameter controlling the amplitude of the added noise. The full description of our data then become:
        \begin{align}\label{eq:datadescription}
            \vec{y} &= f(x,y) + \eta\mathcal{N}(0,1) \nonumber \\
            &= f(\vec{x}) + \epsilon
        \end{align}
        where $\vec{x} = (x,y)$. 
        From section \Sec{regression} we have a model for this data: $\tilde{\vec{y}} = X\hat{\svec{\beta}}$ where $X$ is the design matrix and $\hat{\svec{\beta}}$ are the optimal parameters which we are trying to determine. 

        We visualise the data both with and without noise. Figure \Fig{franke_function} shows the Franke function without any noise ($\eta=0$), plotted for uniformly distributed $x$ and $y$, where $N=20$. We could use more data points, but for the sake computational efficiency we use a $20\cross20$ grid throughout this analysis.  Figure \Fig{franke_function_noise} show the same function, for the same points but now with an added noise of $\eta=0.1$.

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/data3D_no_noise.pdf}
            \caption{The Franke function plotted on a grid where $N=20$ and $\eta=0$. Since we scale the data, the $z$ becomes arbitrary and we choose to leave it out. The important information is the shape of the graph which is smooth when there is no noise. }
            \label{fig:franke_function}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/data3D.pdf}
            \caption{The Franke function with added noise plotted on a grid where $N=20$ and $\eta=0.1$. Since we scale the data, the $z$ becomes arbitrary and we choose to leave it out. The important information is the shape of the graph which becomes quite ragged even for this small level of noise}
            \label{fig:franke_function_noise}
        \end{figure}

    \subsection{Data splitting}\label{sec:splitting}
        In order to test our estimate of $\hat{\svec{\beta}}$ we reserve some of the data for testing. We thus divide our data set into a part for testing a part for training. We select 80 \% of the data for training and the remaining 20 \% for testing the data. The data is split at random. 




    \subsection{Model and design matrix}\label{sec:model}
        The design matrix $X$ has dimensionality ($n\cross p$) where $n$ represents the data points and $p$ the features of the model. We have already split the data set into training and testing, and must therefore create two design matrices: $X^{\text{train}}$ and $X^{\text{test}}$. 
        \\
        We want our model to be a two-dimensional polynomial of order $d$:
        \begin{align*}
            P_d(x,y) = \beta_0 + \sum_{l=1}^d\sum_{k=0}^{l} \beta_jx^{l-k}y^k
        \end{align*}
        where $j\in[1,p]$ and $p=(d+1)\cdot\left[(d+2)/2\right]$ is the number of features, or number of terms in a two dimensional polynomial. $\beta_0$ is our intercept (constant term). From this we set up the design matrix with the following terms (THIS IS WRONG; CORRECT THIS):
        \begin{align}\label{eq:designmatrixequation}
            X_{ij} = \sum_{l=1}^d\sum_{k=0}^{l} x^{l-k}y^k
        \end{align}
        The design matrix is set up without an intercept (constant term) (WHY?). Since we need to set up to design matrices, they account for a different amount of data points, and thus $n$ will be different between the two, but $p$ is the same. 





    
    \subsection{Scaling}\label{sec:scaling}
        We want to scale both our data and the design matrices because the data obtained from either the Franke function, or from some other source often vary a great deal in magnitude. Since the design matrices are set up in order to fit data to a polynomial of degree $d$ in two dimensions, we want to be sure that no term is given more emphasis than the others. In addition, when working with multi-dimensional data and design matrices we want to standardize the features as much as possible to ensure equal (relative) emphasize and treatment of the dimensions. We use the scaling technique \textit{standardization} or \textit{Z-score normalization} which makes the mean of each feature equal to zero, and the their variances to be of unit length. For our observed data $\vec{y}$ this mean:
        \begin{align*}
            \vec{y}' = \frac{\vec{y}-\mu_{\vec{y}}}{\sigma_{\vec{y}}}
        \end{align*}
        where $\vec{y}'$ is now our scaled data. For the design matrices, this must be done for each feature, i.e. for each column.  Mathematically, if $X_j$ is column $j$ of the original design matrix $X$, $j\in[1,p]$:
        \begin{align*}
            X_j' = \frac{X_j-\mu_{X_j}}{\sigma_{X_j}}.
        \end{align*}
        We do this for all the columns and end up with the scaled design matrix $X'$. Since $\optbeta$ is a function of the design matrix of the training data: $X^{\text{train}}$ and the data $\vec{y}$, we need to scale both the training and test data with respect to the mean and standard deviation of each column of the train data:
        \begin{align*}
            X_j'^{\text{ train}} &= \frac{X_j^{\text{train}}-\mu_{X_J^\text{train}}}{\sigma_{X_j^{\text{train}}}} \\
            X_j'^{\text{ test}} &= \frac{X_j^{\text{test}}-\mu_{X_J^\text{train}}}{\sigma_{X_j^{\text{train}}}}
        \end{align*}
        This scaling will ensure that we treat the data in the same way, no matter the actual numerical value of the data. 

    % \begin{align*}
    %     \optbeta' = \optbeta\frac{\sigma_X}{\sigma_{\vec{y}}} \implies \optbeta = \optbeta'\frac{\sigma_{\vec{y}}}{\sigma_X}
    % \end{align*}


    \subsection{Regression analysis for Franke function}\label{sec:reganalysis_franke}

        \subsubsection{OLS}\label{sec:olsanalysis}
            We start by performing an Ordinary Least Square regression analysis, as outlined in section \Sec{OLS}, where we find $\hat{\svec{\beta}}^\text{OLS}$ from \Eq{optimal_beta_ols} and $\variance{\optbeta^{\text{OLS}}}$ from \Eq{variance_of_optimal_beta_ols}. The error can be estimated with the standard deviation $\vec{\sigma}_{\beta} = \variance{\optbeta^{\text{OLS}}}^{1/2}$. 

            We have scaled the data and removed the intercept, thus we do not concern ourselves with $\beta_0$, so $\svec{\beta}^{\text{OLS}} = (\beta_1, \beta_2, \beta_3, \dots, \beta_{p-1})$. We train models, i.e. we find $\optbeta^{\text{OLS}}$ for polynomials up to order $5$. The values of these optimal parameters are plotted in \Fig{beta_with_standard_deviation}, with error bars of one standard deviation. We notice that the variation of the feature parameters increase as the polynomial degree of the model increase. This is expected because when the complexity of the model increase, it want to traverse more points exactly. We further see that the coefficients of the same features more or less have the same sign for the different polynomial degrees. This is a sign of a stable model, as we do not want these coefficient to change much when we use different polynomial degrees for our model. WHY??

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/beta_ols.pdf}
                \caption{Numerical value of the feature parameters $\beta$, with 1$\sigma$ error bars, for polynomials up to order $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$). We note that the parameters for similar features tend to have the same sign across models, however there is an increase in parameter magnitude. This is clearly visible for $d=5$. }
                \label{fig:beta_with_standard_deviation}
            \end{figure}

            Having found the optimal parameter $\optbeta^{\text{OLS}}$ we can make predictions by computing $\tilde{\vec{y}} = X\optbeta^{\text{OLS}}$ on both the training data and the test data. In order to say something about the quality of these predictions we compute the mean square error and the $R^2$ score from equations \Eq{MSE} and \Eq{R2} respectively. The result of this is shown in \Fig{mse_and_r2_for_order5}. We see that the MSE decreases and the $R^2$ score increases (towards 1) as the polynomial degree increase. This signifies that the data we are trying to model (the Franke function) shows a complex behaviour, and is not very well modelled by a low-degree polynomial. From this plot a lone it is fair to deduce that the higher the polynomial degree the better the model. However, we will see that this is not necessarily the case. 

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/MSE_R2_scores_ols.pdf}
                \caption{MSE and $R^2$ scores for train and test data for polynomials up to order $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$). We  see that for the polynomial degrees present, the MSE decrease, while the $R^2$ increase towards 1 for increasing polynomial  degree. }
                \label{fig:mse_and_r2_for_order5}
            \end{figure}
            
            
            We want to investigate the effect of the noise parameter $\eta$ on the MSE for our model. In \Fig{mse_for_different_noise_ols} we have plotted this for $\eta = 10^{\gamma}, \gamma\in[-4,-3,-2,-1,0]$. We observe that for large noise ($\eta=1$) the MSE is high, which is expected. For lower noise the MSE is lower (pay attention to the logarithmic scale for the MSE). The MSE also decreases with polynomial degree for low noise, emphasizing what we found in \Fig{mse_and_r2_for_order5}. In order to represent something slightly more physical than a smooth curve (e.g. terrain data), we will use $\eta=0.1$ for our further analysis. 

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/error_vs_noise_ols.pdf}
                \caption{MSE for train and test data for polynomials up to order $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$), for different noise parameters $\eta = 10^{\gamma}, \gamma\in[-4,-3,-2,-1,0]$. The MSE is large for noisy function (large $\eta$), but decreasing with polynomial degree.}
                \label{fig:mse_for_different_noise_ols}
            \end{figure}


            From \Fig{mse_and_r2_for_order5} we got the impression the higher the polynomial degree, the better the model. We want to investigate this further and look at how the (MEAN) MSE behaves for polynomials of order up to 20. The model is trained on the training data, but the MSE is evaluated for both the training and test data. The result is shown in \Fig{model_complexity_ols}. The MSE of the training data decrease as the polynomial degree increase. This is not a surprise, since the model will traverse more points of the training data exactly as the complexity ($d$) increase. The MSE of the testing data however, seem to decrease at first, but then rapidly increase as the polynomial degree increase. This is similarly explained with the model being too tailored to the training data. When applied to the test data (which consist of completely different pieces of data), the model fails to make accurate predictions. This is an indicator of a phenomenon called \textit{overfitting}: when the model is so tailored to the training data it is unable to make accurate predictions on other, similar data. For a more reliable result, the data in figure \Fig{model_complexity_ols} is generated by bootstrapping the training data with $k=400$ as explained in \Sec{bootstrap}. We consider 400 a suitable number of bootstraps in order to gain an accurate result without requiring too much computational time. Without showing it here, a larger number of bootstraps yield more or less the same results. NANNA CONFIRM THIS. 

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/MSE_ols_BS.pdf}
                \caption{MSE for train and test data for polynomials up to order $d=12$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$) generated with a $k=400$ bootstrap. When we increase the model complexity  up to order $d=12$ we easily  spot that the MSE for train data starts to diverge, which could be a sign of overfitting.}
                \label{fig:model_complexity_ols}
            \end{figure}

            We take the analysis further and consider the bias-variance trade off, as explained in \Sec{bias_variance_tradeoff}. The result is shown in figure \Fig{bias_variance_ols}  where we clearly see that for low polynomial degrees the error occurs mostly from bias, and for high degrees from  variance. There is a trade off, where the  total error switches  main dependence from bias to variance, ultimately minimizing it. In \Fig{bias_variance_ols} this  trade off can be found around polynomial degree $d\approx 5$.

            
            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/tradeoff_ols.pdf}
                \caption{Bias, variance and residual error for polynomials up to order $d=12$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$), generated with a $k=400$ bootstrap. We see that for low polynomial degrees there is a high bias and low variance, the opposite for larger degrees, and we find a trade off at around $d\approx 5$.}
                \label{fig:bias_variance_ols}
            \end{figure}
            
            
            A similar reasoning can be made by applying cross validation as a resampling technique, as explained in \Sec{k_fold}. The result of this is shown in \Fig{cross-validation_ols} where we have used an 8-fold cross validation of the training data and found the corresponding MSE for the training and test data. From the graph we see that MSE for the training data decrease with polynomial degree, which is expected w.r.t. to previous discussion. Similarly the MSE for the test data reaches a minimum and seem to diverge. The minimum point is found at $d\approx 5$.

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/MSE_ols_CV.pdf}
                \caption{MSE for train and test data for polynomials up to order $d=12$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$) generated with an $8-$fold cross validation. We observe a similar behaviour with \Fig{model_complexity_ols} where the MSE for the test data reaches a minimum at $d=5$ and then seem to diverge. }
                \label{fig:cross-validation_ols}
            \end{figure} 
            
            It has become apparent that the polynomial degree that seem to fit our data the best is $d=5$, and so we want to investigate this model further. Considering the bootstrapping resampling technique we make a histogram of the average MSE with samples from each iteration for $d=5$. The  result is shown  in figure \Fig{mse_hist_ols}. We may interpret these histograms (if they were to be normalised) as a probability distribution, where the expectation value of the MSE for the train and test data is the MSE which corresponds to the highest probability. As discussed in section \Sec{bootstrap}, if $k\to\infty$ then this probability distribution approaches the normal distribution and the expectation value would be the mean. Translating this to the specific case at hand here, the most likely MSE value can interpreted as the MSE value with the highest frequency for both the train and test data. If we compare this plot to \Fig{model_complexity_ols} we see that the numerical values of MSEs coincide, with the test MSE being slightly higher than the test MSE. 

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/MSE_hist_OLS.pdf}
                \caption{Histogram of average MSE for train and test data for polynomial degree $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$), generated with a $k=400$ bootstrap. The train data seem to have a slightly lower MSE than the test data. This is consistent with \Fig{model_complexity_ols} for $d=5$.}
                \label{fig:mse_hist_ols}
            \end{figure}

            We also investigate how the mean of the feature parameters $\svec{\beta}$ behave while bootstrapping for $d=5$. Since both the design matrix $X$ and the data $\vec{y}$ are scaled to have mean $\mu=0$, we also expect the mean of the feature parameters to be centred around 0, as emphasized in figure \Fig{beta_with_standard_deviation} where 0 appear to be the by-eye mean.  The resulting histogram is shown in figure \Fig{beta_hist_ols}, where we see a frequency distribution centred at around 0, as expected. 

            \begin{figure}
                \includegraphics[width=\linewidth]{Franke/beta_hist_OLS.pdf}
                \caption{Histogram of average feature parameters $\svec{\beta}$ for polynomial degree $d=5$ for the OLS analysis of the Franke function ($N=20$, $\eta=0.1$), generated with a $k=400$ bootstrap. Since all the features are scaled to mean $\mu=0$, we expect the average features parameter to be close to 0, which is consistent both with this histogram, but also with \Fig{beta_with_standard_deviation} for $d=5$. }
                \label{fig:beta_hist_ols}
            \end{figure}
            


        

        \subsubsection{Ridge}\label{sec:rigdeanalysis}

        We continue our analysis of the model using $d=5$, but now for ridge regression, where the free parameter will be the penalty term $\lambda$. The technicalities of this method is explained in section \Sec{Ridge}. A natural place to start this analysis is to see how the MSE change for different penalty parameters $\lambda$, and we to this by using the two resampling techniques previously discussed. In \Fig{cross-validation_ridge} we see how the MSE for both the train and the test data change as a function of $\lambda$. This plot was generated using an 8-fold cross validation. Since $\lambda=0$ correspond to OLS analysis, we would expect the MSEs for low $\lambda$ to be fairly similar to the OLS values. If we compare this with \Fig{model_complexity_ols} and \Fig{cross-validation_ols} we see that this is indeed the case when $d=5$. We also observe that both the test and train MSEs increase when we increase $\lambda$. 

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/MSE_ridge_CV.pdf}
            \caption{MSE for train and test data for polynomial of degree $d=5$, as function of the penalty parameter $\lambda$ for the ridge analysis of the Franke function ($N=20, \eta=0.1$) generated with an 8-fold cross validation. For small $\lambda$ the train MSE and test MSE are close the OLS values, but both increase when we increase $\lambda$.}
            \label{fig:cross-validation_ridge}
        \end{figure}

        However, when we repeat the analysis but generate the plot using a $k=400$ bootstrap instead, as shown in \Fig{bootstrapping_ridge}, we see that the test MSE decrease to a minimum and remain low when we increase $\lambda$. We also note that the $\lambda$ which minimised the test MSE is found numerically to be $\lambda^\mathrm{ridge}_\mathrm{min} = 8.86\cdot 10^{-4}$ given to two significant figures. 


        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/MSE_ridge_BS.pdf}
            \caption{MSE for train and test data for polynomial of degree $d=5$, as a function of the penalty parameter $\lambda$ for the ridge analysis of the Franke function ($N=20, \eta=0.1$) generated using a $k=400$ bootstrap. We see that the train MSE increase in a similar way as in \Fig{cross-validation_ridge}, while the test MSE decreases to a minimum and remain low, reaching a minimum at around $\lambda\approx 10^{-4}$ }
            \label{fig:bootstrapping_ridge}
        \end{figure}

        We perform the bias-variance analysis with a $k=400$ bootstrap which results in the plot shown in figure \Fig{bias_variance_ridge}. We see that the variance remains low for this $\lambda$ range, while the residual error follows the bias closely. They both seem to reach a minimum at around $\lambda\approx 10^{-4}$ which coincides very well with $\lambda^\mathrm{ridge}_\mathrm{min}$. This is expected since ridge regression is a biased model. 

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/tradeoff_ridge.pdf}
            \caption{Bias, variance and residual error for polynomial of degree $d=5$, as a function of the penalty parameter $\lambda$ for the ridge analysis of the Franke function ($N=20, \eta=0.1$) generated with a $k=400$ bootstrap. We see that the variance remain low for this rang, and the bias and error terms follow each other. They both seem to reach a local minimum at $\lambda\approx 10^{-4}$.}
            \label{fig:bias_variance_ridge}
        \end{figure}

        We investigate the model further by using $\lambda^\mathrm{ridge}_\mathrm{min}$ and plot the frequencies of the average MSE for the train and test data as a histogram using a $k=400$ bootstrap, shown in \Fig{mse_hist_ridge}. From this we see that the test MSE is slightly lower that the train MSE which is in accordance with \Fig{bootstrapping_ridge}

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/MSE_hist_ridge.pdf}
            \caption{Histogram of average MSE for train and test data for polynomial degree $d=5$ for the ridge analysis of the Franke function ($N=20$, $\eta=0.1$), generated with a $k=400$ bootstrap and $\lambda=8.86\cdot 10^{-4}$. The test data has a lower MSE than the train data. This is consistent with \Fig{bootstrapping_ridge} for this value of $\lambda$.}
            \label{fig:mse_hist_ridge}
        \end{figure}

        We may also plot the plot the histogram of the average of the feature parameters $\svec{\beta}$. We would perhaps guess that these values should be centered around 0, using similar arguments as for \Fig{beta_hist_ols}. However, the penalty parameters $\lambda$ will shrink $\svec{\beta}$-values corresponding to small singular values in the design matrix $X$. It therefore give emphasize to the $\svec{\beta}$-values corresponding to large singular values of the design matrix. This may explain the skew towards negative parameter values which we observe in \Fig{beta_hist_ridge} where we plot the abovementioned histogram. 
        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/beta_hist_ridge.pdf}
            \caption{Histogram of average feature parameters $\svec{\beta}$ for polynomial degree $d=5$ for the ridge analysis of the Franke function ($N=20$, $\eta=0.1$), generated with a $k=400$ bootstrap and $\lambda=8.86\cdot 10^{-4}$. With the same reasoning as in \Fig{beta_hist_ols} we would expect these to be centred around 0. However, the penalty parameter shrinks certain beta values, which may explain the skew observed here.}
            \label{fig:beta_hist_ridge}
        \end{figure}










        \subsubsection{Lasso}\label{sec:lassoanalysis}

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/MSE_lasso_CV.pdf}
            \caption{Cross-validation for Lasso regression.}
            \label{fig:cross-validation_lasso}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/MSE_lasso_BS.pdf}
            \caption{Model complexity for Lasso regression. (RIKTIG PLOT?)}
            \label{fig:bootstrapping_lasso}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/tradeoff_lasso.pdf}
            \caption{Bias-variance for Lasso regression.}
            \label{fig:bias_variance_lasso}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{Franke/MSE_heatmap_lasso_CV.pdf}
            \caption{The heatmap shows the cross-validation error resulting from a grid search. FIXME}
            \label{fig:gridsearch_cv_lasso}
        \end{figure}


    % \subsection{Applying best fit model}\label{sec:applybestmodel}


    \subsection{Regression analysis for real terrain data}\label{sec:reganalysis_real_data}

    By the courtesy of \citep{EarthExplorer}, we were able to extract the terrain data of a part of the Crand Ganyon in Arizona, United States. To properly retrieve the geographical coordinates of the map is somewhat circumstantial and utterly unimportant to this task, we to not mind these in this project. We read the GeoTIFF and choose an area from which we extract a coarse grid. In particular, the original cutout area in question consists of $N=900$ points in the $x$ and $y$ direction each. In order to speed up our codes, we use only the data from every \nth{30} $x$- and $y$-point, leaving us with $N=30$ in each direction. We now have $N^2=900$ $z$-points representing relative altitude, of which 20\% are saved for verification. After splitting randomly, we scale the data using the Z-score normalisation explained in \Sec{scaling}. It would not make sense to concern ourselves with the vertical units (the actual altitude), as we already have omitted the horizontal units. The scaled data is what we aspire to work with and is presented in its completeness in \Fig{gc_data}.

    \begin{figure}
        \includegraphics[width=\linewidth]{terrain/data3D.pdf}
        \caption{Scaled terrain data representing a part of the Grand Canyon.}
        \label{fig:gc_data}
    \end{figure}

        \subsubsection{OLS}\label{sec:gc_olsanalysis}

        \begin{figure}
            \includegraphics[width=\linewidth]{terrain/beta_ols.pdf}
            \caption{FIXME.}
            \label{fig:gc_beta_with_standard_deviation}
        \end{figure}


        \begin{figure}
            \includegraphics[width=\linewidth]{terrain/MSE_ols_BS.pdf}
            \caption{Bootstrap training and prediction errors FIXME.}
            \label{fig:gc_model_complexity}
        \end{figure}

        \begin{figure}
            \includegraphics[width=\linewidth]{terrain/tradeoff_ols.pdf}
            \caption{FIXME.}
            \label{fig:gc_bias_variance_ols}
        \end{figure}


        \subsection{Ridge}\label{sec:gc_ridgeanalysis}

        % something about lower limig lambda?
        





        \subsection{Lasso}\label{sec:gc_lassoanalysis}
      





    
\section{Theory}\label{sec:theroy}



Throughout this project we concern ourselves with some observed values $\vec{y}$ for which we seek to obtain an approximation $\tilde{\vec{y}}$ which predicts the true value. Once we have created a model $\tilde{\vec{y}}$ we need to determine its accuracy somehow. There are numerous way of doing this, we will mostly use the Mean Squared Error (MSE)\footnote{Describe MSE briefly} described in \Eq{MSE}, and the R2-score\footnote{describe R2-score briefly}, described in \Eq{R2}.


\begin{align}\label{eq:MSE}
    \MSE{\vec{y}, \tilde{\vec{y}}}= \frac{1}{n}(\vec{y}-\tilde{\vec{y}})^2= \frac{1}{n}\sum_{i=0}^{n-1} (y_i - \tilde{y}_i)^2
\end{align}

\begin{align}\label{eq:R2}
    \Rtwo {\vec{y}, \tilde{\vec{y}}} =1 - \frac{(\vec{y}-\tilde{\vec{y}})^2}{(\vec{y}(1-\bar{y}))^2}= 1 - \frac{\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2}{\sum_{i=0}^{n-1}(y_i-\bar{y})^2}
\end{align}
where the mean of the observed values $\vec{y}$ is given by:
\begin{align*}
    \bar{y} = \frac{1}{n}\sum_{i=0}^{n-1} y_i
\end{align*}



\subsection{Linear regression}\label{sec:regression}
We will attempt to create a model $\tilde{\vec{y}}$ by the means of linear regression. 
There are several possible estimation techniques when fitting a linear regression model. We will discuss three common approaches, one least squares estimation (\Sec{OLS}) and two forms of penalized estimation (\Sec{Ridge} and \Sec{Lasso}).

We assume the vector $\vec{y} \in \RR[n]$ consisting of $n$ observed values $y_i$ to take the form $\vec{y}=f(\vec{x})+\svec{\epsilon}$ where $f(\vec{x})\in\RR[n]$ is a continous function and $\svec{\epsilon}\in \RR[n] $ is a normally distributed noise of standard deviation $\sigma$. We approximate $f$ by $\tilde{\vec{y}}=X\svec{\beta}$, where $X\in \RR[n\cross p]$ is the design matrix of $n$ row vectors $\vec{x}_i\in \RR[p]$, and $\svec{\beta}\in \RR[p]$ are the unknown parameters to be determined. That is, we assume a \textit{linear} relationship between $X$ and $\vec{y}$. The integers $n$ and $p$ then represent the number of data points and features, respectively. 

For an observed value $y_i$ we have $y_i = \vec{x}_i\TT \svec{\beta} + \epsilon_i = \Xbi+ \epsilon_i$. The inner product $\Xbi$ is non-stochastic, hence 

\begin{align*}
    \EE{\Xbi} = \Xbi
\end{align*}

and since 

\begin{align*}
    \EE{\epsilon_i} \stackrel{\text{per def.}}{=} 0,
\end{align*}

we have the expectation value of the response variable

\begin{align*}
    \EE{y_i} &= \EE{\Xbi+ \epsilon_i} \\
    &= \EE{\Xbi} + \EE{\epsilon_i} \\
    &= \Xbi.
\end{align*}

To find the variance of this dependent variable, we need the expetation value of the outer product $\vec{y}\vec{y}\TT$,

\begin{align}
    \EE{\vec{y} \vec{y}\TT} &= \EE{(X\svec{\beta} + \svec{\epsilon})(X\svec{\beta} + \svec{\epsilon})\TT} \nonumber\\
    &= \EE{\Xb \svec{\beta}\TT X\TT + \Xb \svec{\epsilon}\TT + \svec{\epsilon}\svec{\beta}\TT X\TT + \svec{\epsilon} \svec{\epsilon}\TT} \nonumber \\
    &= \Xb \svec{\beta}\TT X\TT + \II \sigma^2. \label{eq:expectation_yyT}
\end{align}

The variance becomes

\begin{align*}
    \variance{y_i} &= \EE{(\vec{y}\vec{y}\TT)_{ii}} -\Big(\EE{y_i}\Big)^2\\
    &= \Xbi \Xbi + \sigma^2 - \Xbi \Xbi\\
    &= \sigma^2.
\end{align*}


The optimal estimator of the coefficients $\svec{\beta}_j$, call it $\optbeta$, is in principle obtained by minimizing the cost function $C(\svec{\beta})$. The cost function is a measure of how badly our model deviates from the observed values, and the method we choose is defined from its cost function. By minimizing it we obtain $\optbeta$, that is:

\begin{align}\label{eq:general_LS}
    \pdv{C(\svec{\beta})}{\svec{\beta}}\Bigg|_{\svec{\beta}=\optbeta} = 0.
\end{align}



\subsubsection{Ordinary Least Squares (OLS)}\label{sec:OLS}

The ordinary least squares (OLS) method assumes the cost function

\begin{align*}
    C^\text{OLS}(\svec{\beta}) = \sum_{i=0}^{n-1}(y_i - \tilde{y}_i)^2 =  \norm{\vec{y}-\tilde{\vec{y}}}_2^{2} = \norm{\vec{y}-\Xb}_2^{2},
\end{align*}

where the subscript "2" implies the \lnorm{2}\footnotemark. Solving \Eq{general_LS} for $C=C^\text{OLS}$ yields the OLS expression for the optimal parameter
\footnotetext{Euclidian norm (\lnorm{2}) is defined as $\norm{\vec{a}}_2 = \sqrt{\sum_ia_i^2}$}

\begin{align}\label{eq:optimal_beta_ols}
    \optbeta^\text{OLS} = \invhessian X\TT \vec{y} = H^{-1} X\TT \vec{y},
\end{align}

where $H = \hessian$ is the Hessian matrix.

Letting $\optbeta = \optbeta^\text{OLS}$ we get the expected value 

\begin{align*}
    \EE{\optbeta} &= \EE{\invhessian X\TT \vec{y}} \\
    &= \invhessian X\TT \EE{\vec{y}} \\
    &= \invhessian \hessian \svec{\beta} \\
    &= \svec{\beta}.
\end{align*}

The variance is then 

\begin{align}\label{eq:variance_of_optimal_beta_ols}\nonumber 
    \variance{\optbeta} &= \EE{\optbeta \optbeta\TT} -\EE{\optbeta} \EE{\optbeta\TT} \nonumber \\
    &= \EE{\invhessian X\TT  \vec{y} \vec{y}\TT X (\invhessian)\TT} - \svec{\beta} \svec{\beta}\TT \nonumber \\
    &= \invhessian X\TT \EE{ \vec{y} \vec{y}\TT } X \invhessian - \svec{\beta} \svec{\beta}\TT \nonumber \\
    &\stackrel{\text{\eqref{eq:expectation_yyT}}}{=}\invhessian X\TT ( X\svec{\beta} \svec{\beta}\TT X\TT+ \II\sigma^2) X \invhessian \nonumber \\
    &= \svec{\beta} \svec{\beta} \TT + \invhessian X\TT \sigma^2 X \invhessian - \svec{\beta} \svec{\beta} \TT \nonumber \\
    &= \sigma^2 \invhessian.
\end{align}



\dots



\subsubsection{Ridge regression}\label{sec:Ridge}

Let $\lambda \in \RR$ be some small number such that $\lambda >0$. If we add a penalty term $\lambda \norm{\svec{\beta}}_2^2$ to the OLS cost function, we get the cost function of Ridge regression,

\begin{align*}
    C^\text{Ridge}(\svec{\beta}) &=  C^\text{OLS}(\svec{\beta}) + \lambda \norm{\svec{\beta}}_2^2 \\
    &=\norm{\vec{y}-\tilde{\vec{y}}}_2^{2}  + \lambda \norm{\svec{\beta}}_2^2 \\
    &= \norm{\vec{y}-\Xb}_2^{2} + \lambda \norm{\svec{\beta}}_2^2 
\end{align*}

\begin{align*}
    \optbeta^\text{Ridge} = \big(\hessian + \lambda \II\big)^{-1} \vec{y} = \big(H + \lambda \II\big)^{-1} \vec{y}
\end{align*}


\subsubsection{Lasso regression}\label{sec:Lasso}

If we add the penalty term $\lambda \norm{\svec{\beta}}_1$, now using the \lnorm{1}\footnotemark, to the OLS cost function, we are left with the Lasso regression's cost function,
\footnotetext{Manhatten norm (\lnorm{1}) is defined as $\norm{\vec{a}}_1 = \sum_i\abs{a_i}$}

\begin{align*}
    C^\text{Lasso}(\svec{\beta})  &= C^\text{OLS}(\svec{\beta}) + \lambda \norm{\svec{\beta}}_1 \\
    &= \norm{\vec{y}-\tilde{\vec{y}}}_2^{2}  + \lambda \norm{\svec{\beta}}_{1} \\
    &= \norm{\vec{y}-\Xb}_2^{2} + \lambda \norm{\svec{\beta}}_{1}
\end{align*}

\begin{align*}
    \optbeta^\text{Lasso} = \dots
\end{align*}

\subsection{Resampling}\label{sec:resampling}
Having obtained some optimal parameters $\optbeta$ from either OLS, Ridge regression or Lasso regression it is of interest to determine how good of a prediction $\optbeta$ yields. Data is often limited and thus we resample the data in clever ways in order to test it for larger samples. We will consider two way of resampling data, the Bootstrap and Cross Validation. 

\subsubsection{Bootstrap method}\label{sec:bootstrap}
We assume $\optbeta$ to be a random variable and it can thus be described a probability density function $p(\vec{t})$. The goal of the Bootstrap is to estimate $p(\vec{t})$. 


\subsubsection{Cross-validation}\label{sec:k_fold}

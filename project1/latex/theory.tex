\section{Theory}\label{sec:theroy}



Throughout this project we concern ourselves with some observed values $\vec{y}$ for which we seek to obtain an approximation $\tilde{\vec{y}}$ which predicts the true value. Once we have created a model $\tilde{\vec{y}}$ we need to determine its accuracy somehow. There are numerous way of doing this, we will mostly use the Mean Squared Error (MSE)\footnote{Describe MSE briefly} described in \Eq{MSE}, and the R2-score\footnote{describe R2-score briefly}, described in \Eq{R2}.


\begin{align}\label{eq:MSE}
    \MSE{\vec{y}, \tilde{\vec{y}}}= \frac{1}{n}\sum_{i=0}^{n-1} (y_i - \tilde{y}_i)^2
\end{align}

\begin{align}\label{eq:R2}
    \Rtwo {\vec{y}, \tilde{\vec{y}}} = 1 - \frac{\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2}{\sum_{i=0}^{n-1}(y_i-\bar{y})^2}
\end{align}
where the mean of the observed values $\vec{y}$ is given by:
\begin{align*}
    \bar{y} = \frac{1}{n}\sum_{i=0}^{n-1} y_i
\end{align*}


Before we delve into the various methods, lets have a look at some mathematical concepts that will be of great use in the further discussion. 

\subsection{Singular value decomposition}\label{sec:SVD}
    The singular value decomposition is a result from linear algebra that states that an arbitrary matrix $A$ of rank $r$ and size $n\cross p$ can be decomposed into the following\cite{svdecomp}:
    \begin{align}\label{eq:svd}
        A = U\Sigma V\TT,
    \end{align}
    where $\Sigma$ is a $n\cross p$ diagonal matrix with the singular values of $A$ as diagonal elements in descending order: $\sigma_1\geq\sigma_2\geq\sigma_3\geq\dots\geq\sigma_r\geq 0$. That is:
    \begin{align*}
        \Sigma = 
        \begin{bmatrix}
            D & 0 \\
            0 & 0
        \end{bmatrix},
        \quad\quad D = 
        \begin{bmatrix}
            \sigma_1 & \dots & 0 \\
            \vdots & \ddots & \vdots \\
            0 & \dots & \sigma_r
        \end{bmatrix}
    \end{align*}
    where $D$ is a diagonal matrix of size $r\cross r$ and $r\leq \text{min}(n,p)$. Further is $U$ a $n\cross n$ matrix whose first $r$ columns is an orthornormal basis of $\text{Col}A$. The remaining columns span $\text{Nul}A\TT$. Alltogether, $U$ forms an orthonormal basis set spanning $\RR[n]$. Likwise, $V$ is a $p\cross p$ square matrix whose columns are an orthonormal basis spanning $\RR[p]$. The first $r$ columns of $V$ form an orthonormal basis of $\text{Row} A$. The ramaining columns span $\text{Nul}A$. As a result of the orthogonality of $U$ and $V$ we have that $U\TT U$ = $VV\TT = \II$

\subsection{Linear regression}\label{sec:regression}
We will attempt to create a model $\tilde{\vec{y}}$ by the means of linear regression. 
There are several possible estimation techniques when fitting a linear regression model. We will discuss three common approaches, one least squares estimation (\Sec{OLS}) and two forms of penalized estimation (\Sec{Ridge} and \Sec{Lasso}).

We assume the vector $\vec{y} \in \RR[n]$ consisting of $n$ observed values $y_i$ to take the form:
\begin{align*}
    \vec{y}=f(\vec{x})+\svec{\epsilon}
\end{align*} where $f(\vec{x})\in\RR[n]$ is a continous function and $\svec{\epsilon}=\eta\mathcal{N}(\mu,\sigma)\in \RR[n] $ is a normally distributed noise of mean $\mu=0$ and standard deviation $\sigma$ and with an amplitude tuning parameter $\eta$. 

We approximate $f$ by $\tilde{\vec{y}}=X\svec{\beta}$, where $X\in \RR[n\cross p]$ is a design matrix of $n$ row vectors $\vec{x}_i\in \RR[p]$, and $\svec{\beta}\in \RR[p]$ are the unknown parameters to be determined. That is, we assume a \textit{linear} relationship between $X$ and $\vec{y}$. The integers $n$ and $p$ then represent the number of data points and features, respectively. 

For an observed value $y_i$ we have $y_i = \vec{x}_i\TT \svec{\beta} + \epsilon_i = \Xbi+ \epsilon_i$. The inner product $\Xbi$ is non-stochastic, hence its expectation value is:

\begin{align*}
    \EE{\Xbi} = \Xbi
\end{align*}

and since 

\begin{align*}
    \EE{\epsilon_i} \stackrel{\text{per def.}}{=} 0,
\end{align*}

we have the expectation value of the response variable as:

\begin{align*}
    \EE{y_i} &= \EE{\Xbi+ \epsilon_i} \\
    &= \EE{\Xbi} + \EE{\epsilon_i} \\
    &= \Xbi.
\end{align*}

To find the variance of this dependent variable, we need the expetation value of the outer product $\vec{y}\vec{y}\TT$,

\begin{align}
    \EE{\vec{y} \vec{y}\TT} &= \EE{(X\svec{\beta} + \svec{\epsilon})(X\svec{\beta} + \svec{\epsilon})\TT} \nonumber\\
    &= \EE{\Xb \svec{\beta}\TT X\TT + \Xb \svec{\epsilon}\TT + \svec{\epsilon}\svec{\beta}\TT X\TT + \svec{\epsilon} \svec{\epsilon}\TT} \nonumber \\
    &= \Xb \svec{\beta}\TT X\TT + \II \sigma^2. \label{eq:expectation_yyT}
\end{align}

The variance now becomes

\begin{align*}
    \variance{y_i} &= \EE{(\vec{y}\vec{y}\TT)_{ii}} -\Big(\EE{y_i}\Big)^2\\
    &= \Xbi \Xbi + \sigma^2 - \Xbi \Xbi\\
    &= \sigma^2.
\end{align*}


The optimal estimator of the coefficients $\svec{\beta}_j$, call it $\optbeta$, is in principle obtained by minimizing the cost function $C(\svec{\beta})$. The cost function is a measure of how badly our model deviates from the observed values, and the method we choose is defined from its cost function. By minimizing it we obtain $\optbeta$, that is:

\begin{align}
    \pdv{C(\svec{\beta})}{\svec{\beta}}\Bigg|_{\svec{\beta}=\optbeta} = 0.
    \label{eq:general_LS}
\end{align}



\subsubsection{Ordinary Least Squares (OLS)}\label{sec:OLS}

The ordinary least squares (OLS) method assumes the cost function

\begin{align*}
    C^\text{OLS}(\svec{\beta}) = \sum_{i=0}^{n-1}(y_i - \tilde{y}_i)^2 =  \norm{\vec{y}-\tilde{\vec{y}}}_2^{2} = \norm{\vec{y}-\Xb}_2^{2},
\end{align*}
where the subscript "2" implies the \lnorm{2}\footnotemark. Solving \Eq{general_LS} for $C=C^\text{OLS}$ yields the OLS expression for the optimal parameter.
\footnotetext{Euclidian norm (\lnorm{2}) is defined as $\norm{\vec{a}}_2 = \sqrt{\sum_ia_i^2}$}

\begin{align}\label{eq:optimal_beta_ols}
    \optbeta^\text{OLS} = \invhessian X\TT \vec{y} = H^{-1} X\TT \vec{y},
\end{align}
where $H = \hessian$ is the Hessian matrix.
Letting $\optbeta = \optbeta^\text{OLS}$ we get the expected value 

\begin{align*}
    \EE{\optbeta} &= \EE{\invhessian X\TT \vec{y}} \\
    &= \invhessian X\TT \EE{\vec{y}} \\
    &= \invhessian \hessian \svec{\beta} \\
    &= \svec{\beta}.
\end{align*}

The variance is then 

\begin{align}\label{eq:variance_of_optimal_beta_ols}
    \variance{\optbeta} &= \EE{\optbeta \optbeta\TT} -\EE{\optbeta} \EE{\optbeta\TT} \nonumber \\
    &= \EE{\invhessian X\TT  \vec{y} \vec{y}\TT X (\invhessian)\TT} - \svec{\beta} \svec{\beta}\TT \nonumber \\
    &= \invhessian X\TT \EE{ \vec{y} \vec{y}\TT } X \invhessian - \svec{\beta} \svec{\beta}\TT \nonumber \\
    &\stackrel{\text{\eqref{eq:expectation_yyT}}}{=}\invhessian X\TT ( X\svec{\beta} \svec{\beta}\TT X\TT+ \II\sigma^2) X \invhessian \nonumber \\
    &= \svec{\beta} \svec{\beta} \TT + \invhessian X\TT \sigma^2 X \invhessian - \svec{\beta} \svec{\beta} \TT \nonumber \\
    &= \sigma^2 \invhessian.
\end{align}

If we perform the singular value decomposition from \Sec{SVD}, and rewrite $X$ using \Eq{svd} we obtain the following:
\begin{align}\label{eq:ols_svd}
    \tilde{\vec{y}} &= X\optbeta = X\invhessian X\TT \vec{y} \nonumber\\
    &= U\S\VT(V\ST\UT U\S\VT)^{-1}V\ST\UT\vec{y} \nonumber\\
    &= U\S\VT(V\S^2\VT)^{-1}V\ST\UT\vec{y}\nonumber \\
    &= U\S\VT V(\S^2)^{-1}\VT V\ST\UT\vec{y}\nonumber \\
    &= U\S^2(\S^2)^{-1}\VT V \VT V\UT\vec{y} \nonumber\\
    &= U\text{diag}\left(\frac{\sigma_i^2}{\sigma_i^2}\right)\UT\vec{y} = U\UT\vec{y} = \sum_{i=1}^p\vec{u}_i\vec{u}\TT_i\vec{y},
\end{align}
where we have used that $\UT U = V\VT = \II$ and $(\VT V)^{-1} = \VT V \implies \VT V \VT V = \II$. 


\subsubsection{Ridge regression}\label{sec:Ridge}

Let $\lambda \in \RR$ be some small number such that $\lambda >0$. If we add a penalty term $\lambda \norm{\svec{\beta}}_2^2$ to the OLS cost function, we get the cost function of Ridge regression,

\begin{align*}
    C^\text{Ridge}(\svec{\beta}) &=  C^\text{OLS}(\svec{\beta}) + \lambda \norm{\svec{\beta}}_2^2 \\
    &=\norm{\vec{y}-\tilde{\vec{y}}}_2^{2}  + \lambda \norm{\svec{\beta}}_2^2 \\
    &= \norm{\vec{y}-\Xb}_2^{2} + \lambda \norm{\svec{\beta}}_2^2 
\end{align*}

ADD SOME FILLER TEXT HERE

\begin{align*}
    \optbeta^\text{Ridge} = \big(\hessian + \lambda \II\big)^{-1} X\TT\vec{y}
\end{align*}
We use the singular value decomposition to obtain a similar result as for OLS. We notice the only difference is the following term:
\begin{align*}
    \big(\hessian + \lambda\II\big)^{-1} &= \big(V\ST\UT U\S\VT + \lambda\II\big)^{-1}\\
    &= \big(V\ST\S\VT + \lambda\II\big)^{-1} \\
    &=\big(V\S^2\VT + \lambda\II\big)^{-1} \\
    &= \big(V\left[\S^2+\lambda\II\right]\VT)^{-1}
\end{align*}
We follow the same argument as with \Eq{ols_svd} and obtain:

\begin{align}\label{eq:ridge_svd}
    \tilde{\vec{y}}^{\text{Ridge}} &=X\optbeta^{\text{Ridge}} \nonumber \\
    &= U\S\VT \big(V\left[\S^2+\lambda\II\right]\VT)^{-1} V\ST\UT\vec{y}\nonumber  \\
    &= U\S^2\left[\S^2+\lambda\II\right]^{-1} \VT V\VT V\UT\vec{y}\nonumber  \\
    &= U\text{diag}\left(\frac{\sigma_i^2}{\sigma_i^2+\lambda}\right)\UT\vec{y} \nonumber \\
    &= \sum_{i=1}^p\vec{u}_i\frac{\sigma_i^2}{\sigma_i^2+\lambda}\vec{u}\TT_i\vec{y}.
\end{align}

If we now compare \Eq{ols_svd} to \Eq{ridge_svd} we see that they are fairly similar but the prediction $\tilde{\vec{y}}^{\text{ridge}}$ contains a factor $\sigma_i^2/(\sigma_i^2 + \lambda)$ where $\sigma_i$ are the singular values of the design matrix $X$ (follows from the singular value decomposition, section \Sec{SVD}), and $\lambda$ is the penalty term which effectively shrinks the predicted values. The shrinkage is large when $\sigma_i^2$ is small, and thus adding this penalty term means that we are emphasizing the parts of $X$ (and thereby the prediction), whose corresponding singular values are the largest. This is called \textit{principal component analysis}.

CAN ADD MORE HERE IF WE WANT


\subsubsection{Lasso regression}\label{sec:Lasso}

If we add the penalty term $\lambda \norm{\svec{\beta}}_1$, now using the \lnorm{1}\footnotemark, to the OLS cost function, we are left with the Lasso regression's cost function,
\footnotetext{Manhatten norm (\lnorm{1}) is defined as $\norm{\vec{a}}_1 = \sum_i\abs{a_i}$}

\begin{align*}
    C^\text{Lasso}(\svec{\beta})  &= C^\text{OLS}(\svec{\beta}) + \lambda \norm{\svec{\beta}}_1 \\
    &= \norm{\vec{y}-\tilde{\vec{y}}}_2^{2}  + \lambda \norm{\svec{\beta}}_{1} \\
    &= \norm{\vec{y}-\Xb}_2^{2} + \lambda \norm{\svec{\beta}}_{1}
\end{align*}

The analytical expression for $\optbeta^{\text{Lasso}}$ is not trivial to derive, and when performing the analysis we will use the \texttt{scikit-learn} package in python. Hence, we do not derive this analytical expression. 

\subsection{Resampling}\label{sec:resampling}
Having obtained some optimal parameters $\optbeta$ from either OLS, Ridge regression or Lasso regression it is of interest to determine how good of a prediction $\optbeta$ yields. Data is often limited and thus we resample the data in clever ways in order to test it for larger samples. We will consider two ways of resampling data, the Bootstrap and Cross Validation. 

\subsubsection{Bootstrap method}\label{sec:bootstrap}
Suppose we have some set of data $\vec{y}$ from which we have estimated $\optbeta$. We think of $\svec{\beta}$ as a random variable (since $\svec{\beta}=\svec{\beta}(X)$) with an unknown probability distribution $p(\vec{\svec{\beta}})$, that we want to estimate. We then have that $\optbeta$ is the $\svec{\beta}$ that has the highest probability. We do the following:
\begin{enumerate}
    \item From the data $\vec{y}$ we draw with replacement as many numbers as there are in $\vec{y}$ and create a new dataset $\vec{y}^*$.
    \item We then estimate $\svec{\beta}^*$ by using the data in $\vec{y}^*$. 
    \item Repeat this $k$ times and we are left with a set of vectors $B = (\svec{\beta}^*_1, \svec{\beta}^*_2, \dots, \svec{\beta}^*_k)$ The relative frequency of vectors $\svec{\beta}^*$ in $B$ is our approximation of $p(\svec{\beta})$. 
\end{enumerate}
We now have a collection of $k$ $\svec{\beta}$ parameters. If we assume $y$ to be independent and identically distributed variables, the central limit theorem tells us that the distribution of $\svec{\beta}$ parameters should approach a normal distribution when $k$ is sufficiently large. Thus, $\optbeta$, which is the beta with the highest probability should approach the expectation value of the above distribution, which for a normal distribution is just the mean values. We therefore write:
\begin{align*}
    \optbeta^* = \EE{\svec{\beta^*}} = \bar{B} 
\end{align*}
which is our estimate of the optimal parameter $\optbeta^*$ after bootstrapping. From the set of vectors $B$ we can estimate the variance and standard error of $\svec{\beta}$, both of which will be vector quantities, with entries that corresponds to each feature in our model. 

\subsubsection{Cross-validation}\label{sec:k_fold}
Another resampling technique is the cross-validation. Suppose we have the data set $\vec{y}$ which we split into $k$ smaller datasets equal in size. Then:
\begin{enumerate}
    \item Decide on one (or more) of the sets to be the testing test. The remaining sets will be considered the training set.
    \item Fit some model to the training set. Evaluate this model by finding the desired test scores. This could be the $MSE$ and/or $R^2$ scores. Save these values on discard the model. 
    \item Repeat $k$ times, or until all the data have been used as test data. 
\end{enumerate}
We use the retained scores for all the testing sets in our assessment of the model. 

\section{Theory}\label{sec:theroy}

\hl{*} DECIDE ON z vs. y

\begin{align}\label{eq:MSE}
    \MSE{\vec{y}, \tilde{\vec{y}}}= \frac{1}{n}(\vec{y}-\tilde{\vec{y}})^2= \frac{1}{n}\sum_{i=0}^{n-1} (y_i - \tilde{y}_i)^2
\end{align}

\begin{align}\label{eq:R2}
    \Rtwo {\vec{y}, \tilde{\vec{y}}} =1 - \frac{(\vec{y}-\tilde{\vec{y}})^2}{(\vec{y}(1-\bar{y}))^2}= 1 - \frac{\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2}{\sum_{i=0}^{n-1}(y_i-\bar{y})^2}
\end{align}

\begin{align*}
    \bar{y} = \frac{1}{n}\sum_{i=0}^{n-1} y_i
\end{align*}



\subsection{Regression}\label{sec:regression}

We assume the vector $\vec{y} \in \RR[n]$ consisting of observed values $y_i$ to take the form $\vec{y}=f(\vec{x})+\svec{\epsilon}$ where $f(\vec{x})\in\RR[n]$ is a continous function and $\svec{\epsilon}\in \RR[n] $ is a normally distributed noise. We approximate $f$ by $\tilde{\vec{y}}=X\svec{\beta}$, where $X\in \RR[n\cross p]$ is the design matrix of $n$ row vectors $\vec{x}_i\in \RR[p]$, and $\svec{\beta}\in \RR[p]$ are the unknown parameters to be determined. The integers $n$ and $p$ represent the number of data points and features, respectively. 



For an observed value $y_i$ we have $y_i = \vec{x}_i\TT \svec{\beta} + \epsilon_i = \Xbi+ \epsilon_i$\footnote{Can also write $X_{ij}\beta_j$, but we try to be consistent with our notation.}. The inner product $\vec{x}_i\TT \svec{\beta}$ is non-stochastic, hence 

\begin{align*}
    \EE{\Xbi} = \Xbi
\end{align*}

and since 

\begin{align*}
    \EE{\epsilon_i} \stackrel{\text{per def.}}{=} 0,
\end{align*}

we have the expectation value

\begin{align*}
    \EE{y_i} &= \EE{\Xbi+ \epsilon_i} \\
    &= \EE{\Xbi} + \EE{\epsilon_i} \\
    &= \Xbi.
\end{align*}

To find the variance, we need the expetation value of the outer product $\vec{y}\vec{y}\TT$,

\begin{align}
    \EE{\vec{y} \vec{y}\TT} &= \EE{(X\svec{\beta} + \svec{\epsilon})(X\svec{\beta} + \svec{\epsilon})\TT} \nonumber\\
    &= \EE{\Xb \svec{\beta}\TT X\TT + \Xb \svec{\epsilon}\TT + \svec{\epsilon}\svec{\beta}\TT X\TT + \svec{\epsilon} \svec{\epsilon}\TT} \nonumber \\
    &= \Xb \svec{\beta}\TT X\TT + \II \sigma^2. \label{eq:expectation_yyT}
\end{align}

The variance becomes

\begin{align*}
    \variance{y_i} &= \EE{(\vec{y}\vec{y}\TT)_{ii}} -\Big(\EE{y_i}\Big)^2\\
    &= \Xbi \Xbi + \sigma^2 - \Xbi \Xbi\\
    &= \sigma^2.
\end{align*}


Taking $\optbeta$ as the Ordinary Least Square (OLS) expression for the optimal parameter, i.e.

\begin{align*}
    \optbeta = \invhessian X\TT \vec{y},
\end{align*}

we get the expected value

\begin{align*}
    \EE{\optbeta} &= \EE{\invhessian X\TT \vec{y}} \\
    &= \invhessian X\TT \EE{\vec{y}} \\
    &= \invhessian \hessian \svec{\beta} \\
    &= \svec{\beta}.
\end{align*}

The variance is then 

\begin{align*}
    \variance{\optbeta} &= \EE{\optbeta \optbeta\TT} -\EE{\optbeta} \EE{\optbeta\TT} \\
    &= \EE{\invhessian X\TT  \vec{y} \vec{y}\TT X (\invhessian)\TT} - \svec{\beta} \svec{\beta}\TT \\
    &= \invhessian X\TT \EE{ \vec{y} \vec{y}\TT } X \invhessian - \svec{\beta} \svec{\beta}\TT \\
    &\stackrel{\text{\eqref{eq:expectation_yyT}}}{=}\invhessian X\TT ( X\svec{\beta} \svec{\beta}\TT X\TT+ \II\sigma^2) X \invhessian \\
    &= \svec{\beta} \svec{\beta} \TT + \invhessian X\TT \sigma^2 X \invhessian - \svec{\beta} \svec{\beta} \TT \\
    &= \sigma^2 \invhessian.
\end{align*}



\subsubsection{Ordinary Least Squares (OLS)}\label{sec:OLS}


\subsubsection{Ridge regression}\label{sec:Ridge}


\subsubsection{Lasso regression}\label{sec:Lasso}


\subsection{Resampling}\label{sec:resampling}

\subsubsection{Bootstrap method}\label{sec:bootstrap}

\subsubsection{Cross-validation}\label{sec:k_fold}
